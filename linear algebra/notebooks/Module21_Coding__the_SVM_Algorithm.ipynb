{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <center><strong>Important:</strong> Make a Copy of this Google Colab Notebook!\n",
        "</center>\n",
        "\n",
        "<p>Please refrain from using or modifying this current Google Colab notebook directly. Instead, follow these instructions to create your own copy:</p>\n",
        "\n",
        "<ol>\n",
        "  <li>Go to the \"File\" menu at the top of the Colab interface.</li>\n",
        "  <li>Select \"Save a copy in Drive\" to create a duplicate of this notebook.</li>\n",
        "  <li>You can now work on your own copy without affecting the original.</li>\n",
        "</ol>\n",
        "\n",
        "<p>This ensures that you have a personalized version to work on and make changes according to your needs. Remember to save your progress as you go. Enjoy working on your own copy of the Google Colab notebook!</p>"
      ],
      "metadata": {
        "id": "M_xkV_Z9sRL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Module 21 Coding the Support Vector Machine Algorithm**\n",
        "In this module, you will apply your knowledge from the past couple of modules to implement SVM using Python. The goal is to write the algorithm from scratch using Python for the task of binary classification. <p>\n",
        "\n",
        "## **Getting Started**\n",
        "\n",
        "Run the provided code sections and follow the instructions. Implement your own code were indicated."
      ],
      "metadata": {
        "id": "tPWSySrGsgRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Importing Python Packages**\n",
        "The first step is to import your necessary Python packages."
      ],
      "metadata": {
        "id": "NivuMlhpsoOU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ke3AbLf8ukso"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # for handling multi-dimensional array operation\n",
        "import pandas as pd  # for reading data from csv\n",
        "from sklearn.preprocessing import MinMaxScaler  # for normalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn import datasets # Importing the datasets available from scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading and Inspecting the Dataset**\n"
      ],
      "metadata": {
        "id": "m6jzpIOLAj5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will load and inspect a dataset from scikit-learn. This dataset contains information about breast cancer tumors from digitized images of fine needle aspirates (FNA) of breast masses. This is a commonly used dataset for binary classification tasks, where the objective is to predict whether a tumor is malignant (cancerous) or benign (non-cancerous).\n",
        "\n",
        "Within the dataset, there are 30 numerical features including the mean, standard error, and worst (largest) values of attributes such as radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, and fractal dimension.\n",
        "\n",
        "The target variable represents the diagnosis of the tumor and is encoded as follows:\n",
        "*   0: Malignant (indicating the presence of cancer)\n",
        "*   1: Benign (indicating the absence of cancer)"
      ],
      "metadata": {
        "id": "xSZaX293IAaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.io.common import dataclasses\n",
        "cancer_data = datasets.load_breast_cancer()\n",
        "\n",
        "data = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\n",
        "\n",
        "# Add the target variable to the DataFrame\n",
        "data['diagnosis'] = cancer_data.target\n",
        "diagnosis_map = {1:1, 0:-1} # Changing the values from 1:Malignant and 0:Benign to 1:Maligant and -1:Benign\n",
        "data['diagnosis'] = data['diagnosis'].map(diagnosis_map)"
      ],
      "metadata": {
        "id": "hpWOgaVBuaiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will use `data.describe()` to inspect the features within the dataset and their distributions. These summary statistics show count, mean, standard deviation, minimum, maximum, and quartile information for numeric columns. This helps in understanding the central tendencies and distributions of the data. Inspecting these distributions is also useful for determining any inaccuracies within the dataset. For instance, in this dataset we have many metrics of size including the mean radius, mean perimeter, and mean area. We expect the minimum value for these features to be greater than zero since they are describing distances. By inspecting the data prior to running the algorithm, we can correct for any dataset errors."
      ],
      "metadata": {
        "id": "ItRDj8fnBtjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViyJh3Eu_GiJ",
        "outputId": "32849bba-31a6-47f7-b009-2aae351f8b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       mean radius  mean texture  mean perimeter    mean area  \\\n",
            "count   569.000000    569.000000      569.000000   569.000000   \n",
            "mean     14.127292     19.289649       91.969033   654.889104   \n",
            "std       3.524049      4.301036       24.298981   351.914129   \n",
            "min       6.981000      9.710000       43.790000   143.500000   \n",
            "25%      11.700000     16.170000       75.170000   420.300000   \n",
            "50%      13.370000     18.840000       86.240000   551.100000   \n",
            "75%      15.780000     21.800000      104.100000   782.700000   \n",
            "max      28.110000     39.280000      188.500000  2501.000000   \n",
            "\n",
            "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
            "count       569.000000        569.000000      569.000000           569.000000   \n",
            "mean          0.096360          0.104341        0.088799             0.048919   \n",
            "std           0.014064          0.052813        0.079720             0.038803   \n",
            "min           0.052630          0.019380        0.000000             0.000000   \n",
            "25%           0.086370          0.064920        0.029560             0.020310   \n",
            "50%           0.095870          0.092630        0.061540             0.033500   \n",
            "75%           0.105300          0.130400        0.130700             0.074000   \n",
            "max           0.163400          0.345400        0.426800             0.201200   \n",
            "\n",
            "       mean symmetry  mean fractal dimension  ...  worst texture  \\\n",
            "count     569.000000              569.000000  ...     569.000000   \n",
            "mean        0.181162                0.062798  ...      25.677223   \n",
            "std         0.027414                0.007060  ...       6.146258   \n",
            "min         0.106000                0.049960  ...      12.020000   \n",
            "25%         0.161900                0.057700  ...      21.080000   \n",
            "50%         0.179200                0.061540  ...      25.410000   \n",
            "75%         0.195700                0.066120  ...      29.720000   \n",
            "max         0.304000                0.097440  ...      49.540000   \n",
            "\n",
            "       worst perimeter   worst area  worst smoothness  worst compactness  \\\n",
            "count       569.000000   569.000000        569.000000         569.000000   \n",
            "mean        107.261213   880.583128          0.132369           0.254265   \n",
            "std          33.602542   569.356993          0.022832           0.157336   \n",
            "min          50.410000   185.200000          0.071170           0.027290   \n",
            "25%          84.110000   515.300000          0.116600           0.147200   \n",
            "50%          97.660000   686.500000          0.131300           0.211900   \n",
            "75%         125.400000  1084.000000          0.146000           0.339100   \n",
            "max         251.200000  4254.000000          0.222600           1.058000   \n",
            "\n",
            "       worst concavity  worst concave points  worst symmetry  \\\n",
            "count       569.000000            569.000000      569.000000   \n",
            "mean          0.272188              0.114606        0.290076   \n",
            "std           0.208624              0.065732        0.061867   \n",
            "min           0.000000              0.000000        0.156500   \n",
            "25%           0.114500              0.064930        0.250400   \n",
            "50%           0.226700              0.099930        0.282200   \n",
            "75%           0.382900              0.161400        0.317900   \n",
            "max           1.252000              0.291000        0.663800   \n",
            "\n",
            "       worst fractal dimension   diagnosis  \n",
            "count               569.000000  569.000000  \n",
            "mean                  0.083946    0.254833  \n",
            "std                   0.018061    0.967836  \n",
            "min                   0.055040   -1.000000  \n",
            "25%                   0.071460   -1.000000  \n",
            "50%                   0.080040    1.000000  \n",
            "75%                   0.092080    1.000000  \n",
            "max                   0.207500    1.000000  \n",
            "\n",
            "[8 rows x 31 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also inspect the first few rows of the dataset using `data.head(n)` where `n` is the number of rows you want to output. In this case, we will look at the first three rows of data. You can change the value for the number of rows to inspect more or less of the data."
      ],
      "metadata": {
        "id": "H2vmlLjYCXvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMp8IMz6CYmo",
        "outputId": "7d32654e-368d-40bd-a1f6-b4dd742e8ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0        17.99         10.38           122.8     1001.0          0.11840   \n",
            "1        20.57         17.77           132.9     1326.0          0.08474   \n",
            "2        19.69         21.25           130.0     1203.0          0.10960   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0           0.27760          0.3001              0.14710         0.2419   \n",
            "1           0.07864          0.0869              0.07017         0.1812   \n",
            "2           0.15990          0.1974              0.12790         0.2069   \n",
            "\n",
            "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
            "0                 0.07871  ...          17.33            184.6      2019.0   \n",
            "1                 0.05667  ...          23.41            158.8      1956.0   \n",
            "2                 0.05999  ...          25.53            152.5      1709.0   \n",
            "\n",
            "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
            "0            0.1622             0.6656           0.7119                0.2654   \n",
            "1            0.1238             0.1866           0.2416                0.1860   \n",
            "2            0.1444             0.4245           0.4504                0.2430   \n",
            "\n",
            "   worst symmetry  worst fractal dimension  diagnosis  \n",
            "0          0.4601                  0.11890         -1  \n",
            "1          0.2750                  0.08902         -1  \n",
            "2          0.3613                  0.08758         -1  \n",
            "\n",
            "[3 rows x 31 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Engineering and Splitting the Dataset**"
      ],
      "metadata": {
        "id": "TSupkLZgFYsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many times when you are working with data you'll be provided with raw data which may be incompatible with your model or hinder its performance. In this case, we'll need to do some pre-processing, or feature engineering, on the dataset before using our algorithm. For this dataset, we will do feature engineering through normalization. Normalization is the process of converting a range of values, into a standard range of values, typically in the interval [−1, 1] or [0, 1]. It’s not a strict requirement but it improves the speed of learning (e.g. faster convergence in gradient descent) and prevents numerical overflow."
      ],
      "metadata": {
        "id": "SkoV5C0sH6My"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y = data.loc[:, 'diagnosis']  # all rows of 'diagnosis', these are our targets\n",
        "X = data.iloc[:, 1:30]  # all rows of column 1 and ahead, these are our features\n",
        "\n",
        "# normalize the features using MinMaxScalar from sklearn.preprocessing\n",
        "X_normalized = MinMaxScaler().fit_transform(X.values)\n",
        "X = pd.DataFrame(X_normalized)"
      ],
      "metadata": {
        "id": "HMCPWCKdHY1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to split the dataset into our training and testing datasets. This is necessary since we need to retain a testing dataset to see how the model will perform on unseen observations. To split the dataset, we'll use the function `train_test_split()` from sklearn.model_selection. The function takes in the following:\n",
        "*   `X`: The feature data, typically represented as a NumPy array or pandas DataFrame.\n",
        "*   `y`: The target variable or label data, corresponding to the feature data.\n",
        "*   `test_size`: The proportion (between 0.0 and 1.0) of the dataset to include in the test split. For example, `test_size=0.2` would create a test set comprising 20% of the total data, while the remaining 80% is allocated to the training set.\n",
        "*   `random_state`: The seed value used for random shuffling and splitting of the dataset. Setting a specific random_state ensures reproducibility of the split. If random_state is not provided, the data will be split differently each time the function is called.\n",
        "\n",
        "The `train_test_split()` function returns four subsets:\n",
        "*   `X_train`: The training set of feature data.\n",
        "*   `X_test`: The test set of feature data.\n",
        "*   `y_train`: The corresponding target variable for the training set.\n",
        "*   `y_test`: The corresponding target variable for the test set."
      ],
      "metadata": {
        "id": "5Ypxyg3FIF8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first insert 1 in every row for intercept b\n",
        "X.insert(loc=len(X.columns), column='intercept', value=1)\n",
        "\n",
        "# test_size is the portion of data that will go into test set\n",
        "# random_state is the seed used by the random number generator\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "OQHmMVEWI_gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our dataset is ready to be used for binary classification with a support vector machine."
      ],
      "metadata": {
        "id": "ti3AYDTyL8ty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cost Function**"
      ],
      "metadata": {
        "id": "90tH2unUMA9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cost function, or objective function, is one of the main building blocks for our algorithm. This cost function is what we are trying to maximize or minimize depending on the machine learning algorithm. If you continue to learn more about machine learning, you'll encounter a wide variety of cost functions.\n",
        "\n",
        "For SVM, our objective is to find a hyperplane that separates our samples with the highest possible margin while keeping misclassification as low as possible. We can achieve this by **minimizing** the following cost function:\n",
        "\n",
        "$ J(\\textbf{w}) = \\frac{1}{2} \\lVert \\mathbf{w} \\rVert^2 + C \\bigg[ \\frac{1}{N} ∑_{i}^{n} \\text{max} (0,1-y_i * (\\mathbf{w} \\cdot x_i + b)) \\bigg] $\n",
        "\n",
        "The cost function can also be written as:\n",
        "\n",
        "$ J(\\textbf{w}) = \\frac{1}{2} ⋅ λ \\lVert \\mathbf{w} \\rVert ^2 + \\frac{1}{N} ∑_{i}^{n} \\text{max} (0,1-y_i * (\\mathbf{w} \\cdot x_i + b)) $\n",
        "\n",
        "Using the cost function directly above, we can see that $\\lambda$ is essentially equal to $1/C$ which means that a larger $\\lambda$ will give a wider margin. Either of these cost functions can be used for SVM but it's important to remember what each regularization parameter (either $C$ or $\\lambda$) does and then making adjustments to these paramters as necessary.\n",
        "\n",
        "For our cost function, let's use the first function listed above. We'll start by computing the cost before moving on to its gradient which is what we'll be using for training.\n"
      ],
      "metadata": {
        "id": "yV-N8_RQMDnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(W, X, Y):\n",
        "    # calculate hinge loss\n",
        "    N = X.shape[0]\n",
        "    distances = 1 - Y * (np.dot(X, W))\n",
        "    distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
        "    hinge_loss = reg_strength * (np.sum(distances) / N)\n",
        "\n",
        "    # calculate cost\n",
        "    cost = 1 / 2 * np.dot(W, W) + hinge_loss\n",
        "    return cost"
      ],
      "metadata": {
        "id": "pcj20IVkUNYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this cost function, you may notice that the intercept term $b$ is missing. In this example, the term has been included in the weight vector as follows:\n",
        "\n",
        "$ f(x_i) = \\tilde{w} \\cdot \\tilde{x_i} + w_0 = \\textbf{w} \\cdot x_i$\n",
        "\n",
        "where $\\textbf{w} = (\\tilde{w},w_0), x_i = (\\tilde{x}_i,1)$\n",
        "\n",
        "This is why we added the extra column of 1s before splitting the dataset."
      ],
      "metadata": {
        "id": "GimdT99IUdLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gradient of Cost Function**"
      ],
      "metadata": {
        "id": "eyYllZdNVvXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to compute the gradient of the cost function. Using our cost function, we can compute the gradient as:\n",
        "\n",
        "$ \\nabla J(\\textbf{w}) = \\frac{1}{N} \\sum_i^n \\begin{cases}\n",
        "    \\textbf{w} & \\text{if max } (0,1-y_i * (\\textbf{w} \\cdot x_i)) = 0 \\\\\n",
        "    \\textbf{w} - C y_i x_i & \\text{otherwise}\n",
        "\\end{cases} $"
      ],
      "metadata": {
        "id": "gQF83h25VyLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cost_gradient(W, X_batch, Y_batch):\n",
        "    Y_batch = np.array([Y_batch])\n",
        "    X_batch = np.array([X_batch])\n",
        "\n",
        "    distance = 1 - (Y_batch * np.dot(X_batch, W))\n",
        "    dw = np.zeros(len(W))\n",
        "\n",
        "    for ind, d in enumerate(distance):\n",
        "        if max(0, d) == 0:\n",
        "            di = W\n",
        "        else:\n",
        "            di = W - (reg_strength * Y_batch[ind] * X_batch[ind])\n",
        "        dw += di\n",
        "\n",
        "    dw = dw/len(Y_batch)  # average\n",
        "    return dw"
      ],
      "metadata": {
        "id": "ZKSqCu67XSdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training using Stochastic Gradient Descent (SGD)**"
      ],
      "metadata": {
        "id": "ERNE5N_lXbh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the SVM algorithm, we need to minimize our cost function. This cost function is essentially a masure of how bad our model is performing. To find the minimum of $J(\\textbf{w})$ we need to minimize $\\lVert \\mathbf{w} \\rVert ^2$ which maximizes the margin ($2/\\lVert \\mathbf{w} \\rVert$), and we need to minimize the sum of hinge loss which minimizes the misclassifications. The hinge loss function is: $ \\text{max} (0,1-y_i * (w \\cdot x_i + b))$. We minimize the cost function since both of our SVM objectives are achieved through minimization.\n",
        "\n",
        "In order to minimize the cost function we wil be using Stochastic Gradient Descent (SGD). SGD is visually represented in the figure below.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/faq/gradient-optimization/ball.png\" alt=\"Image Description\">\n",
        "\n",
        "The gradient descent algorithm works as follows:\n",
        "1.   Find the gradient of the cost function\n",
        "2.   Move opposite to the gradient by a certain rate\n",
        "3.   Repeat until convergence\n",
        "\n",
        "Moving in the opposite direction to the gradient works because the gradient is the direction of fastest increase of the function. In order to reach the minimum of the function, we want to move in the opposite of that direction. This is why the fuction is called gradient **descent**. In the case of typical gradient descent, the first step is computed using all samples.\n",
        "\n",
        "In our case, we want to use stochastic gradient descent which means that we are using a single example of the data at a time. This is beneficial since this method is efficient and converges quickly.\n",
        "\n"
      ],
      "metadata": {
        "id": "IzkWCg71XfN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(features, outputs):\n",
        "    #max_epochs = 5000\n",
        "    weights = np.zeros(features.shape[1])\n",
        "    # stochastic gradient descent\n",
        "    for epoch in range(1, max_epochs):\n",
        "        # shuffle to prevent repeating update cycles\n",
        "        X, Y = shuffle(features, outputs)\n",
        "        for ind, x in enumerate(X):\n",
        "            ascent = calculate_cost_gradient(weights, x, Y[ind])\n",
        "            weights = weights - (learning_rate * ascent)\n",
        "\n",
        "    return weights"
      ],
      "metadata": {
        "id": "rmHp84AJaHg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training the Model**"
      ],
      "metadata": {
        "id": "3l7PMAcvoNPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train the model using our SGD function. To train the model, we need to also set values for the regularization strength and the learning rate. The regularization strength controls the trade-off between achieving a low training error and minimizing the complexity of the SVM model. The learning rate refers to a hyperparameter that controls the step size or rate at which the model parameters are updated during each iteration of the optimization process."
      ],
      "metadata": {
        "id": "t9RlyNG7oQHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg_strength = 10000 # regularization strength\n",
        "learning_rate = 0.000001 # step size\n",
        "max_epochs = 5000 # number of times we do gradient descent\n",
        "\n",
        "W = sgd(X_train.to_numpy(), y_train.to_numpy())\n",
        "print(\"weights are: {}\".format(W))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BnA82IFlcBY",
        "outputId": "b34fb427-8af8-4c89-8b3d-5d08e15b6505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights are: [-8.52781948e-01 -1.49560499e+00 -2.47602200e+00  1.24570577e+00\n",
            "  3.24101058e+00 -3.25425106e+00 -6.89749508e+00  4.99782945e-01\n",
            " -2.03420387e-03 -5.68536285e+00  1.86156941e+00 -3.24168156e+00\n",
            " -3.79257070e+00 -1.65203504e+00  2.38378528e+00  1.74338092e+00\n",
            " -8.17056535e-01  1.97707839e+00  1.81484724e+00 -2.98566465e+00\n",
            " -5.29938880e+00 -1.27252924e+00 -3.27123761e+00 -2.16394532e+00\n",
            "  6.11456534e-01 -2.60055045e+00 -3.92555214e-02 -4.69075257e+00\n",
            " -2.20470848e+00  9.15814564e+00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing the Model**"
      ],
      "metadata": {
        "id": "W9CFbaPaoHex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can test the model using the weights from training the model. Here we are using these weights to predict if the data for a given tumor is malignant or benign. We can compare the results to the actual known values for these tumors in order to output the accuracy, recall, and precision for the model.\n",
        "\n",
        "*   **Precision**: Precision is a measure of the model's ability to correctly identify positive instances (true positives) out of all instances predicted as positive. It is calculated as the ratio of true positives (TP) to the sum of true positives and false positives (FP).\n",
        "*   **Recall (Sensitivity or True Positive Rate)**: Recall measures the ability of the model to correctly identify positive instances (true positives) out of all actual positive instances. It is calculated as the ratio of true positives (TP) to the sum of true positives and false negatives (FN).\n",
        "*   **Accuracy**: Accuracy measures the overall correctness of the model's predictions. It calculates the ratio of the number of correct predictions (true positives and true negatives) to the total number of instances. Accuracy provides an overall measure of the model's performance, considering both positive and negative predictions. However, accuracy alone may not be sufficient if the dataset is imbalanced (i.e., when the number of instances in one class is much higher than the other), as the model may achieve high accuracy by simply predicting the majority class."
      ],
      "metadata": {
        "id": "ppC_0k01oKXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_predicted = np.array([])\n",
        "for i in range(X_test.shape[0]):\n",
        "    yp = np.sign(np.dot(X_test.to_numpy()[i], W))\n",
        "    y_test_predicted = np.append(y_test_predicted, yp)\n",
        "\n",
        "print(\"Accuracy on test dataset: {}\".format(accuracy_score(y_test, y_test_predicted)))\n",
        "print(\"Recall on test dataset: {}\".format(recall_score(y_test, y_test_predicted)))\n",
        "print(\"Precision on test dataset: {}\".format(precision_score(y_test, y_test_predicted)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J36MsWelinP",
        "outputId": "c9bce1c4-27ad-4e96-bf92-777790de392d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on test dataset: 0.9736842105263158\n",
            "recall on test dataset: 0.9859154929577465\n",
            "precision on test dataset: 0.9722222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Adjusting SVM Parameters**"
      ],
      "metadata": {
        "id": "2msukAr-Q4yY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's make some adjustments to the parameters `reg_strength`, `learning_rate` and `max_epochs`. Change these values and note the changes in the accuracy, recall, and precision.\n",
        "\n",
        "*   What happens when you increase the size of the learning rate?\n",
        "*   What happens if you decrease the maximum number of epochs?\n",
        "*   What happens when you decrease the regularization strength?\n",
        "*   Adjust these three parameters to improve the accuracy, precision, and recall of the model compared to the original parameter values of:\n",
        "```\n",
        "reg_strength = 10000\n",
        "learning_rate = 0.000001\n",
        "max_epochs = 5000\n",
        "```\n",
        "Remember the original accuracy, recall, and precison were:\n",
        "```\n",
        "Accuracy on test dataset: 0.9736842105263158\n",
        "Recall on test dataset: 0.9859154929577465\n",
        "Precision on test dataset: 0.9722222222222222\n",
        "```\n",
        "\n",
        "✅ **Discuss the changes in accuracy, precision, and recall on Piazza or Discord with your fellow classmates!**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3UWPZxi3PvQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✨ **Congratulations you have now coded the SVM algorithm from scratch using Python!** ✨"
      ],
      "metadata": {
        "id": "gNn3BPyAp47A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is from https://towardsdatascience.com/svm-implementation-from-scratch-python-2db2fc52e5c2#7944 which provides additional information on coding the SVM algorithm from scratch."
      ],
      "metadata": {
        "id": "ZJo5ixEpqq6Z"
      }
    }
  ]
}