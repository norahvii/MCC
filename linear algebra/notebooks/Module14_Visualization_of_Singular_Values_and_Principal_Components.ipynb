{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <center><strong>Important:</strong> Make a Copy of this Google Colab Notebook!\n",
        "</center>\n",
        "\n",
        "<p>Please refrain from using or modifying this current Google Colab notebook directly. Instead, follow these instructions to create your own copy:</p>\n",
        "\n",
        "<ol>\n",
        "  <li>Go to the \"File\" menu at the top of the Colab interface.</li>\n",
        "  <li>Select \"Save a copy in Drive\" to create a duplicate of this notebook.</li>\n",
        "  <li>You can now work on your own copy without affecting the original.</li>\n",
        "</ol>\n",
        "\n",
        "<p>This ensures that you have a personalized version to work on and make changes according to your needs. Remember to save your progress as you go. Enjoy working on your own copy of the Google Colab notebook!</p>\n"
      ],
      "metadata": {
        "id": "Uw8Y_S4LSEeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XZHPEcxxa3wf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Module 14: Visualization of Singular Values and Singular Vectors**\n",
        "We will now explore the meaning of singular values and vectors in image compression following [this tutorial](https://scicoding.com/pca-using-python-image-compression/). We will use the sklearn PCA function for this module. However, feel free to use your custom function instead. <p>\n",
        "\n",
        "This practice is divided into 4 Parts, with the last being optional. <p>\n",
        "\n",
        "<ul><li> Part 1: Compress a single gray scale image using PCA </li>\n",
        "<li> Part 2: How any principal components are needed to efficiently compress the image? </li>\n",
        "<li> Part 3:  Singular Vectors and Principal Components </li>\n",
        "<li> [OPTIONAL] Part 4: Image Compression Unsing PCA of an image data set </li> </ul>\n",
        "\n",
        "<i>Note: Depending on your previous experience with Python and familiarity with PCA, Parts 1-3 may feel relatively straightforward. As a result, we have included an optional Part 4 to provide an additional challenge. If you are new to these concepts or prefer to focus on Parts 1-3, skip Part 4.Â  </i> <p>\n",
        "\n",
        "## **Getting Started**\n",
        "\n",
        "Run the provided code sections and follow the instructions. Implement your own code were indicated."
      ],
      "metadata": {
        "id": "28R8MyAjd5mm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing Python Packages**\n",
        "The first step is to import your necessary Python packages. <p>\n"
      ],
      "metadata": {
        "id": "C07k1MskpxQ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKFSar5XgHy3"
      },
      "outputs": [],
      "source": [
        "import numpy as np # Importing the numpy library as np - this is a common practice for Python coding\n",
        "import matplotlib.pyplot as plt # Importing the matplotlib.pyplot as plt - useful for visualizing data\n",
        "import pandas as pd # libary for data management\n",
        "import seaborn as sb # library for visualization\n",
        "\n",
        "import skimage # for image conversion\n",
        "from google.colab import files # upload files from local drive\n",
        "\n",
        "from sklearn.decomposition import PCA # for PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For learning purposes will use the image from the tutorial, later on, you can choose your own image. To do so, download [this image](https://drive.google.com/file/d/1qYL44GQYbjJyaxL9rBO2hPSEB-BSpATx/view?usp=sharing) to your local machine. Next, execute the following code section. When you run this code, it will prompt you to select files from your computer. Choose the image you just downloaded, and it will be transferred to the current working directory in the Colab environment."
      ],
      "metadata": {
        "id": "v7rkclZOM9-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()  # Upload the image file"
      ],
      "metadata": {
        "id": "MvJdtmYQqBsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first uploaded file (assuming only one file was uploaded)\n",
        "file_name = next(iter(uploaded))\n",
        "\n",
        "image = skimage.io.imread(file_name) # Load the image\n",
        "\n",
        "image = skimage.color.rgb2gray(image) # Convert to grayscale"
      ],
      "metadata": {
        "id": "UjL0q-A7NewZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note: To perform Principal Component Analysis (PCA) on a grayscale image, you typically need to convert the image into a 1D vector. PCA operates on a 2D matrix where each row is a sample (an image, in this case) and each column is a feature (a pixel value, in this case).*"
      ],
      "metadata": {
        "id": "68RIsabgU8sz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Compress a single gray scale **image** using PCA\n",
        "We will now explore how to compress an image using PCA using the built-in function, but feel free to use your own PCA function instead.  "
      ],
      "metadata": {
        "id": "G7sNXRPz_Rt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Principal Component Analysis (PCA) with 50 components\n",
        "pca = PCA(n_components=50)  # select number of components\n",
        "image_compressed = pca.fit_transform(image)  # perform PCA on the image data\n",
        "\n",
        "# Reconstruct the image from the compressed representation\n",
        "image_decompressed = pca.inverse_transform(image_compressed)\n",
        "\n",
        "# Plot images\n",
        "fig, axes = plt.subplots(1, 2, figsize=(100, 100))\n",
        "axes[0].imshow(image, cmap='gray')\n",
        "axes[0].set_label(\"Original image\")  # set label for the original image plot\n",
        "axes[1].imshow(image_decompressed, cmap='gray')\n",
        "axes[1].set_label(\"Compressed image\")  # set label for the compressed image plot"
      ],
      "metadata": {
        "id": "fsfdR_c2_JDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<strong>Task:</strong> Explain the difference between the two images. Play around using a different number of principal components. How does this change the resulting image?"
      ],
      "metadata": {
        "id": "AONxYGz0ANQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Principal Component Analysis (PCA) with different numbers of components\n",
        "n_components = ############ Insert your own code here ############   # list of different number of components for compressing the image\n",
        "fig, axes = plt.subplots(1, len(n_components) + 1, figsize=(15, 5))\n",
        "\n",
        "# Plot the original image\n",
        "axes[0].imshow(image, cmap='gray')\n",
        "axes[0].set_xlabel(\"Original image\")\n",
        "\n",
        "for i, n_comp in enumerate(n_components):\n",
        "    # Perform PCA with the current number of components\n",
        "    pca = PCA(n_components=n_comp)\n",
        "    image_compressed = pca.fit_transform(image)\n",
        "    image_decompressed = pca.inverse_transform(image_compressed)\n",
        "\n",
        "    # Plot the compressed image in the corresponding subplot\n",
        "    axes[i+1].imshow(image_decompressed, cmap='gray')\n",
        "    axes[i+1].set_xlabel(\"Compressed image\")\n",
        "    axes[i+1].set_title(\"PCs: {}\".format(n_comp))\n",
        "\n",
        "# Adjust subplot spacing\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_D9rOdkKCEx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: How any principal components are needed to efficiently compress the image?\n",
        "<strong>Task</strong>: Think about the following questions:\n",
        "<ul><li> What do the principal components represent in the context of image compression?\n",
        "<li>How can we interpret the principal components visually? Do they capture specific image features or patterns?\n",
        "<li>Are the principal components directly related to image content or do they represent more abstract features?\n",
        "\n",
        "<h3>Answers:</h3>\n",
        "<ol>\n",
        "  <li><strong>What do the principal components represent in the context of image compression?</strong><br>italicized text\n",
        "    Principal components represent the directions or axes in the image space along which the data varies the most. They capture the most significant patterns or features present in the image data.\n",
        "  </li>\n",
        "  <li><strong>How can we interpret the principal components visually? Do they capture specific image features or patterns?</strong><br>\n",
        "    Principal components can be interpreted visually by examining the singular vectors associated with them. Each principal component represents a combination of image features or patterns. The components with larger singular values capture more pronounced or dominant features, while those with smaller singular values capture subtler or less significant patterns.\n",
        "  </li>\n",
        "  <li><strong>Are the principal components directly related to image content or do they represent more abstract features?</strong><br>\n",
        "    The principal components can represent both specific image content and more abstract features. In some cases, a principal component may correspond to a recognizable object or texture in the image. However, other components may represent more abstract features that are not directly linked to specific objects but capture more global variations or structures in the image.\n",
        "  </li>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "mCaGSNjcAzmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Singular Vectors and Principal Components\n",
        "In this step, we will explore the singular values and principal components.\n",
        "\n",
        "<ul><li><strong>Singular values</strong> (in PCA, also known as the square roots of eigenvalues of the covariance matrix) represent the importance of each principal component in capturing the variance in the data. A larger singular value for a principal component means that component represents a direction in feature space along which there's more variance in the data.\n",
        "\n",
        "<li><strong>Principal components </strong> (eigenvectors of the covariance matrix) represent directions in the feature space along which the data varies the most. They are orthogonal to each other, meaning that they capture non-redundant information in the data. In the transformed space, each data point's coordinates are its projections onto the principal components, indicating how much the data point varies along each of those directions.</ul>\n",
        "\n",
        "This is why PCA is often used for dimensionality reduction: by keeping only the first few principal components (those associated with the largest singular values), we can capture the majority of the variance in the data with fewer dimensions."
      ],
      "metadata": {
        "id": "eIDm76H0BIDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the singular values."
      ],
      "metadata": {
        "id": "mtM-zq9AGoVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Principal Component Analysis (PCA) with 600 components\n",
        "num_components = 600  # select number of components (original image size is 667x1000)\n",
        "pca = ############ Insert your own code here ############\n",
        "image_compressed = ############ Insert your own code here ############\n",
        "\n",
        "# Plot the singular values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(pca.singular_values_)\n",
        "plt.title(\"Singular values\")\n",
        "plt.xlabel(\"Component\")\n",
        "plt.ylabel(\"Singular value\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rZQCZTLVGrLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how much variance the first principal components explain."
      ],
      "metadata": {
        "id": "O3ZKpZys-z-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create subplots with 1 row and 2 columns\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "axes[0].plot(range(1, num_components+1), 100 * np.cumsum(pca.explained_variance_ratio_))\n",
        "axes[0].set_xlabel('Number of Components')\n",
        "axes[0].set_ylabel('Cumulative Explained Variance (%)')\n",
        "axes[0].set_title('Explained Variance by Components')\n",
        "\n",
        "# Plot explained variance per component\n",
        "axes[1].plot(range(1, num_components+1), 100 * pca.explained_variance_ratio_)\n",
        "axes[1].set_xlabel('Component')\n",
        "axes[1].set_ylabel('Explained Variance (%)')\n",
        "axes[1].set_title('Explained Variance per Component')\n",
        "\n",
        "# Set y-axis limits\n",
        "axes[0].set_ylim(50, 110)\n",
        "axes[1].set_ylim(-10, 70)\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "crxJksvTARI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the data table."
      ],
      "metadata": {
        "id": "llu2Gy43BOLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the cumulative explained variance\n",
        "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_) * 100\n",
        "\n",
        "# Create a DataFrame\n",
        "explained_variance = pd.DataFrame({\n",
        "    'Component': range(1, len(cumulative_explained_variance) + 1),\n",
        "    'Explained Variance (%)': cumulative_explained_variance.round(2)\n",
        "})\n",
        "\n",
        "# Display the DataFrame\n",
        "print(explained_variance.head(10)) # display first 10 rows"
      ],
      "metadata": {
        "id": "ah8N9Sro-iIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot a selection of principal components."
      ],
      "metadata": {
        "id": "yM0zmW5qkWfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the eigenvectors (principal components)\n",
        "principal_components = ############ Insert your own code here ############  # hint look into the sklearn documentation\n",
        "\n",
        "# Plot the first few eigenvectors\n",
        "n_vectors = 5  # Number of vectors to plot\n",
        "fig, axes = plt.subplots(n_vectors, 1, figsize=(10, 5))\n",
        "for i in range(n_vectors):\n",
        "    ax = axes[i]\n",
        "    ax.plot(principal_components[i])\n",
        "    ax.set_title(\"Principal Component {}\".format(i+1))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ul-9cIRwPX3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<strong> How to interpret the plots of the eigenvectors / principal components? </strong>\n",
        "\n",
        "When you apply PCA to a single image, each row of pixels in the image is considered a sample, and each pixel in that row is treated as a feature. So, if you plot the principal components, each point on the line represents the weight or \"loading\" of a specific pixel (feature) in the principal component.\n",
        "\n",
        "Here's how you interpret these plots in your context:\n",
        "\n",
        "<ul>\n",
        "    <li><strong>Magnitude:</strong> A larger absolute value of the weight indicates that the intensity of a specific pixel has a larger influence on the principal component. This pixel is a key characteristic that distinguishes this component from others.</li>\n",
        "    <li><strong>Direction:</strong> A positive or negative sign indicates if a pixel's intensity positively or negatively influences the principal component. If two pixels have the same sign, their intensities tend to increase or decrease together in the direction of the principal component.</li>\n",
        "    <li><strong>Pattern:</strong> A flat line indicates that all pixels contribute equally to this principal component, suggesting that this component represents the general brightness of the image. Peaks indicate that certain pixels contribute significantly to this component, suggesting that this component represents a specific pattern in the image.</li>\n",
        "</ul>\n",
        "The first few components often capture large-scale variations in pixel intensities, and later components may capture more subtle patterns or noise. Interpreting these components can be challenging, especially when the original image has complex patterns. However, visualizing the components can often give you an intuitive sense of the patterns that PCA has detected in the image.\n",
        "\n"
      ],
      "metadata": {
        "id": "iuyfWcaUknUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's plot the principal components as a heat map."
      ],
      "metadata": {
        "id": "ufaV9kkTkSqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Let's take the first 10 components\n",
        "n_vectors = 10\n",
        "principal_components_subset = principal_components[:n_vectors, :]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(principal_components_subset, cmap='viridis')\n",
        "plt.title('Heatmap of the First 10 Principal Components')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Principal Components')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yKzIMvDEQ_td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<strong>Explanation of heatmap </strong>\n",
        "\n",
        " When applying Principal Component Analysis (PCA) to a grayscale image, you treat each row as a sample and each pixel in that row as a feature. The principal components derived from PCA are vectors in the feature space, and visualizing them as a heatmap helps understand the contribution of each pixel to these components.\n",
        "\n",
        "In the heatmap, each column represents a pixel (feature) in the image, and the color in a column indicates how much that pixel contributes to each principal component. Consistent colors across components mean the pixel has a similar contribution to all of them. Each row corresponds to a principal component, and the color in a row shows how much each pixel contributes to that component. Intense colors in a row indicate that only a few pixels significantly contribute to that component.\n",
        "\n",
        "The color scale in the heatmap represents the weights of the pixels. Darker colors (towards yellow) indicate higher absolute weights, meaning those pixels are more important in defining the corresponding principal component. The sign (positive or negative) of the weight indicates the direction of influence.\n",
        "\n",
        "For example, a brightly colored cell corresponding to the 3rd component and the 500th pixel means that the 500th pixel in the image has a high weight in the 3rd principal component. Changing the intensity of this pixel would significantly alter the value of the 3rd component.\n",
        "\n",
        "Keep in mind that interpreting the heatmap can be challenging, especially for complex image patterns. However, it provides a way to visually understand the structure in your high-dimensional image data."
      ],
      "metadata": {
        "id": "ihpGocGn8Cry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<strong>Compression Ratio </strong>"
      ],
      "metadata": {
        "id": "mdP9-ZcMBboO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k2XfmESkFFC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the compression ratio\n",
        "original_data_size = image.size\n",
        "\n",
        "# Initialize the compression ratios list\n",
        "compression_ratios = []\n",
        "\n",
        "num_components_vec = list(range(1, 601))\n",
        "\n",
        "# Calculate the compression ratio for each number of components\n",
        "for n in range(1,num_components+1):\n",
        "\n",
        "    # Calculate the compressed data size\n",
        "    compressed_data_size = n*image.shape[1]\n",
        "\n",
        "    # Calculate the compression ratio\n",
        "    compression_ratio = compressed_data_size / original_data_size\n",
        "\n",
        "    # Append the compression ratio to the list\n",
        "    compression_ratios.append(compression_ratio*100)\n",
        "\n",
        "# Plot the compression ratios\n",
        "plt.plot(num_components_vec, compression_ratios)\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Compression Ratio (%)')\n",
        "plt.title('Compression Ratio for Different Numbers of Components')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XPArLcWhDps5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets relate the compression ratio with explained variance."
      ],
      "metadata": {
        "id": "4gL_pHGQGcW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the compression ratios and cumulative variances lists\n",
        "compression_ratios = []\n",
        "cumulative_variances = []\n",
        "\n",
        "num_components_vec = list(range(1, 601))\n",
        "\n",
        "# Calculate the compression ratio and cumulative variance for each number of components\n",
        "for n in num_components_vec:\n",
        "    # Calculate the compressed data size\n",
        "    compressed_data_size = n * image.shape[1]\n",
        "    # Calculate the compression ratio\n",
        "    compression_ratio = compressed_data_size / original_data_size * 100\n",
        "    compression_ratios.append(compression_ratio)\n",
        "\n",
        "    # Calculate the cumulative explained variance\n",
        "    cumulative_variance = np.sum(pca.explained_variance_ratio_[:n]) * 100\n",
        "    cumulative_variances.append(cumulative_variance)\n",
        "\n",
        "# Plot the compression ratios\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(num_components_vec, compression_ratios, label='Compression Ratio')\n",
        "plt.plot(num_components_vec, cumulative_variances, label='Cumulative Explained Variance')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Percentage')\n",
        "plt.title('Compression Ratio vs. Cumulative Explained Variance')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Initialize the variance explained levels list\n",
        "variance_levels = [80, 90, 95, 99, 99.9] # we want to find number of PCs that are close to these levels\n",
        "\n",
        "# Initialize the compression ratios list\n",
        "compression_ratios = []\n",
        "# Initialize list of number of components that explain the variance\n",
        "comps = []\n",
        "# Initialize the list for actually explained variance\n",
        "explained_variance = []\n",
        "\n",
        "# Calculate the compression ratio for each variance explained level\n",
        "for level in variance_levels:\n",
        "    # Find the number of components that explain the specified variance level\n",
        "    num_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= level / 100) + 1\n",
        "    comps.append(num_components)\n",
        "\n",
        "    # Calculate the actual explained variance for the number of components\n",
        "    actual_explained_variance = np.sum(pca.explained_variance_ratio_[:num_components]) * 100\n",
        "    explained_variance.append(actual_explained_variance)\n",
        "\n",
        "    # Calculate the compressed data size\n",
        "    compressed_data_size = num_components * image.shape[1]\n",
        "\n",
        "    # Calculate the compression ratio\n",
        "    compression_ratio = compressed_data_size / original_data_size\n",
        "\n",
        "    # Append the compression ratio to the list\n",
        "    compression_ratios.append(compression_ratio)\n",
        "\n",
        "# Create a DataFrame\n",
        "data = {'PCs': comps,\n",
        "        'Variance Explained (%)': variance_levels,\n",
        "        'Actual Explained Variance (%)': explained_variance,\n",
        "        'Compression Ratio': compression_ratios}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(df.to_string(index=False)) # display datatable in one line"
      ],
      "metadata": {
        "id": "N7aeIx_9G3Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create subplots with 2 rows and len(variance_levels) + 1 columns\n",
        "fig, axes = plt.subplots(1, len(explained_variance) + 1, figsize=(15, 6))\n",
        "\n",
        "# Plot the original image\n",
        "axes[0].imshow(image, cmap='gray')\n",
        "axes[0].set_xlabel(\"Original image\")\n",
        "\n",
        "for i, n_comp in enumerate(comps):\n",
        "    # Perform PCA with the specified variance level\n",
        "    pca = PCA(n_components=n_comp)\n",
        "    image_compressed = pca.fit_transform(image)\n",
        "    image_decompressed = pca.inverse_transform(image_compressed)\n",
        "\n",
        "    # Plot the compressed image and reconstructed image in the corresponding subplots\n",
        "    axes[i+1].imshow(image_decompressed, cmap='gray')\n",
        "    axes[i+1].set_xlabel(\"Var Explained: {}%\".format(explained_variance[i].round(2)))\n",
        "    axes[i+1].set_title(\"PCs: {}\".format(n_comp))\n",
        "\n",
        "# Adjust subplot spacing\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "255aZIXpLrSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<strong>Task</strong>: After going through the visualizations and looking at compression rations vs explained variance, think again about the following questions:\n",
        "<ul><li> How any principal components are needed to efficiently compress the image?\n",
        "<li>What do the principal components represent in the context of image compression?\n",
        "<li>How can we interpret the principal components visually? Do they capture specific image features or patterns?\n",
        "<li>Are the principal components directly related to image content or do they represent more abstract features? </ul>"
      ],
      "metadata": {
        "id": "ujnRFJWiQapl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **[OPTIONAL] Part 4: Image Compression Unsing PCA of an image data set**\n",
        "\n",
        "In many applications you will use PCA to compress a large data set of images. If you are interested in learning more about it, we recommend you to follow  [this tutorial](https://towardsdatascience.com/image-compression-using-principal-component-analysis-pca-253f26740a9f). Here, PCA is used to compress an image data set of handrwitten digits.\n",
        "\n",
        "<h2><i> Note the following differences between performing PCA on a single image vs on a data set of several images </i>\n",
        "\n",
        "Remember, that the input to Principal Component Analysis (PCA) is a dataset represented as a matrix, where each row corresponds to a separate sample ('n_samples') and each column represents a feature ('n_features') [link text](https:// [link text](https://))of that sample.\n",
        "\n",
        "<h2>PCA on a Single Image:</h2>\n",
        "<p>\n",
        "  When applying Principal Component Analysis (PCA) to a single image, the image is treated as a matrix, with each row representing a separate sample ('n_samples') and each pixel in that row considered as a feature ('n_features'). PCA aims to find patterns of maximum variance across the rows of the image. The principal components obtained from PCA are linear combinations of the original features (pixels in a row). These principal components capture the most significant patterns or structures in the image, and the transformed data represents the projection of the original image onto these components.\n",
        "</p>\n",
        "\n",
        "<h2>PCA on a Dataset of Images:</h2>\n",
        "<p>\n",
        "  When applying PCA to a dataset of multiple images, each image is first flattened from a 2D array to a 1D array. Then, each flattened image is considered as a single sample in the dataset. For example, if we have a dataset of 1000 images, each of size (32, 32), we reshape each image into a (1024,) array, resulting in a (1000, 1024) dataset.\n",
        "</p>\n",
        "<p>\n",
        "  In this context, 'n_samples' refers to the number of images in the dataset, and 'n_features' represents the total number of pixels in each image. PCA is then applied to this dataset, seeking directions (principal components) in the high-dimensional image space along which the original images exhibit the most variation. The principal components extracted represent common patterns or features across the set of images. By analyzing the values of the principal components for a specific image, we can determine the extent to which that image displays the patterns encoded by those components.\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "EQ5nUgkuuAiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <center>Well done! Module 14 focused on improving your understanding of PCA by walking through compression of a single gray scale image. We explored the importance of singular values and principal components in understanding how PCA captures the most variance in image data. Visualizing these components deepens this understanding and builds your skills in using PCA for practical applications like reducing data size while maintaining quality.</center>\n"
      ],
      "metadata": {
        "id": "_KFA1jmMZ7EO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Own Code from follwing the tutorial\n",
        "\n",
        "############ Insert your own code here ############"
      ],
      "metadata": {
        "id": "C4hCzEs6Xpgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <center> You are an absolute Pro - well done! </center>"
      ],
      "metadata": {
        "id": "kpu__B9Y2bB8"
      }
    }
  ]
}