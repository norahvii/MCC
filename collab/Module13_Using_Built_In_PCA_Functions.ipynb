{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# <center><strong>Important:</strong> Make a Copy of this Google Colab Notebook!\n","</center>\n","\n","<p>Please refrain from using or modifying this current Google Colab notebook directly. Instead, follow these instructions to create your own copy:</p>\n","\n","<ol>\n","  <li>Go to the \"File\" menu at the top of the Colab interface.</li>\n","  <li>Select \"Save a copy in Drive\" to create a duplicate of this notebook.</li>\n","  <li>You can now work on your own copy without affecting the original.</li>\n","</ol>\n","\n","<p>This ensures that you have a personalized version to work on and make changes according to your needs. Remember to save your progress as you go. Enjoy working on your own copy of the Google Colab notebook!</p>\n"],"metadata":{"id":"Uw8Y_S4LSEeP"}},{"cell_type":"markdown","source":["# **Module 13: Comparison of your own PCA function and Pythons built-in PCA function.** \n","\n","In this module you compare your own PCA function with the built-in of the python sklearn package.\n","\n","## **Getting Started** \n","\n","Run the provided code sections and follow the instructions. Implement your own code were indicated. "],"metadata":{"id":"IPqsidvrgbCy"}},{"cell_type":"markdown","source":["## **Importing Python Packages**\n","The first step is to import your necessary Python packages. <p>\n"," Check out the built in sklearn packages: [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) & [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PC) "],"metadata":{"id":"C07k1MskpxQ4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKFSar5XgHy3"},"outputs":[],"source":["import numpy as np # Importing the numpy library as np - this is a common practice for Python coding\n","import matplotlib.pyplot as plt # Importing the matplotlib.pyplot as plt - useful for visualizing data  \n","import pandas as pd # libary for data management\n","import seaborn as sb # library for visualization"]},{"cell_type":"markdown","source":["## Copy your own PCA function from Module 12 here. "],"metadata":{"id":"dPkAm16ofMf_"}},{"cell_type":"code","source":["def PCA_custom(X, nComponents):\n","\n","\n","\n","  return X_reduced, explained_variance"],"metadata":{"id":"XZ9JfX00motx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Compare your own PCA function with the built-in function using the [IRIS datset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)**\n","\n","Load the data set.\n"],"metadata":{"id":"ntqzzBBFs4Xe"}},{"cell_type":"code","source":["# Get the IRIS dataset\n","url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n","data = pd.read_csv(url, names=['sepal length', 'sepal width', 'petal length', 'petal width', 'target'])\n","\n","# Set the figure size for the plot\n","plt.figure(figsize=(6, 6))\n","\n","# Create a scatter plot using seaborn\n","# - Set the data to data (original dataset)\n","# - Set the x-axis to 'sepal length' and y-axis to 'sepal width'\n","# - Use the 'target' variable for coloring different groups\n","# - Set the marker size to 60\n","# - Use the 'icefire' color palette for coloring\n","sb.scatterplot(data=data, x='sepal length', y='sepal width', hue='target', s=60, palette='icefire')\n"],"metadata":{"id":"nl9DS4J4z74E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Results of your custom function.\n","\n"],"metadata":{"id":"XWzTuSbZ0LVn"}},{"cell_type":"code","source":["# Prepare the data\n","x = data.iloc[:, 0:4]  # Select the features (sepal length, sepal width, petal length, petal width)\n","\n","# Prepare the target\n","target = data.iloc[:, 4]  # Select the target variable (iris species)\n","\n","# Applying Principal Component Analysis (PCA) \n","[mat_reduced, exp_variance ]=PCA_custom(x, 4)  # Your custom function\n","\n","# Creating a Pandas DataFrame of the reduced dataset \n","principal_df = pd.DataFrame(mat_reduced, columns=['PC1', 'PC2', 'PC3', 'PC4'])  # Create a DataFrame with the reduced components\n","\n","# Concatenate the reduced components with the target variable to create a complete dataset\n","principal_df = pd.concat([principal_df, pd.DataFrame(target)], axis=1)  # Concatenate the reduced components and target variable\n","\n","# Print explained variance ratios\n","for i, exp_var in enumerate(exp_variance):\n","    print(f\"Explained Variance for PC{i+1}: {exp_var*100:.2f}%\")"],"metadata":{"id":"grhhVQP2xEik"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's visualize the results. <p>"],"metadata":{"id":"5JI83o1jzNnM"}},{"cell_type":"code","source":["# Check the size of the data\n","print(\"Size of the original data: %s\\nSize of the dimensionality reduced data: %s\" % (x.shape, mat_reduced.shape))\n","\n","# Set the figure size for the plot\n","plt.figure(figsize=(6, 6))\n","\n","# Create a scatter plot using seaborn - not we only display the first 2 PCs\n","sb.scatterplot(data=principal_df, x='PC1', y='PC2', hue='target', s=60, palette='icefire')\n","\n","plt.xlabel(f'PC1: {exp_variance[0]*100:.2f}%')\n","plt.ylabel(f'PC2: {exp_variance[1]*100:.2f}%')"],"metadata":{"id":"BuUJj8iCzVdf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's use the built-in function from the sklearn package to perform PCA. <p>\n","\n","<strong>Tasks:</strong>\n","<ol><li>Run the code below to perform PCA on normalized data (zero mean, unit variance). Does the result differ between your custom code and the built-in function? Why or why not?</li>\n","<li>Look at the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">documentation</a> and check out parameters you can use.</li>\n","<li>Look at the <a href=\"https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/decomposition/_pca.py#L118\">source code</a>. This might be intimidating if you haven't looked at source code before. However, this is good practice as it allows you to see how PCA was implemented. Take notes on the differences between your custom function and the sklearn built-in function.</li>\n","<li>Look at the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale\">documentation</a> and <a href=\"https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/preprocessing/_data.py#L123\">source code</a> of the normalization step using sklearn. Does this differ from the normalization you did?</li>\n","</ol>\n","\n","<i> Note: </i> Take your time when looking at the documentation and source code. The main idea of this module is to built familarity with the sklearn package and reading python docucumentation and source code. Feel free to play around by changing parameters and explore how this effects your results. "],"metadata":{"id":"r8YHuqvn4cW-"}},{"cell_type":"code","source":["# built-ins for PCA\n","from sklearn.decomposition import PCA # PCA built-in from sklearn\n","from sklearn.preprocessing import scale # normalizing built-in sklear \n","\n","x_norm = scale(x, axis = 0); #zero mean, unit variance\n","\n","print(\"Data Mean and Variance before normalizing: \", np.mean(x, axis = 0), np.var(x, axis = 0), \"\\nData Mean and Variance after normalizing: \", np.mean(x_norm, axis = 0), np.var(x_norm, axis = 0), \"\\n\")\n","\n","pca = PCA(n_components=4) #select the number of components for your model\n","\n","principalComponents = pca.fit_transform(x_norm)\n","\n","principal_df2 = pd.DataFrame(data = principalComponents\n","             , columns = ['PC1', 'PC2', 'PC3', 'PC4'])\n","\n","principal_df2 = pd.concat([principal_df2, pd.DataFrame(target)], axis = 1)\n","\n","# Print explained variance ratios\n","for i, exp_var in enumerate(pca.explained_variance_ratio_):\n","    print(f\"Explained Variance for PC{i+1}: {exp_var*100:.2f}%\")\n"],"metadata":{"id":"0TthncoC20jt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's visualize the results. <p>"],"metadata":{"id":"JL7L0ac3VtTh"}},{"cell_type":"code","source":["# Check the size of the data\n","print(\"Size of the original data: %s\\nSize of the dimensionality reduced data: %s\" % (x_norm.shape, mat_reduced.shape))\n","\n","# Set the figure size for the plot\n","plt.figure(figsize=(6, 6))\n","\n","# Create a scatter plot using seaborn - not we only display the first 2 PCs\n","sb.scatterplot(data=principal_df2, x='PC1', y='PC2', hue='target', s=60, palette='icefire')\n","\n","plt.xlabel(f'PC1: {pca.explained_variance_ratio_[0]*100:.2f}%')\n","plt.ylabel(f'PC2: {pca.explained_variance_ratio_[1]*100:.2f}%')"],"metadata":{"id":"oUpa5YVHVpHS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's inspect the data tables of the Principal Components. <p>\n","For your custom PCA code:"],"metadata":{"id":"_9qn2P6QZKeP"}},{"cell_type":"code","source":["principal_df"],"metadata":{"id":"qyU05tirXTkn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For the built in sklearn PCA function."],"metadata":{"id":"ZEUgU8vGZTYf"}},{"cell_type":"code","source":["principal_df2"],"metadata":{"id":"iexDDaZnXVfj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## <center> Module 13 aimed to enhance your understanding of Principal Component Analysis (PCA) using the scikit-learn (sklearn) toolbox. Additionally, it focused on developing your skills in exploring the sklearn documentation and source code to gain deeper insights into the implementation details of PCA. Although looking at source code can be overwhelming initially, it is a beneficial practice as it helps you understand differences between algorithms and improves your coding skills. </center>\n"],"metadata":{"id":"_KFA1jmMZ7EO"}},{"cell_type":"markdown","source":["## **[OPTIONAL] Apply the sklearn PCA Function on Your Own Dataset**\n","\n","<strong>Task</strong>: Start by visualizing the data from your chosen dataset. Then, implement PCA using the sklearn package. Experiment with different choices for the number of principal components to use for dimensionality reduction. Justify your selection of the number of principal components and observe how it impacts your results. Finally, visualize your results. "],"metadata":{"id":"EQ5nUgkuuAiT"}},{"cell_type":"markdown","source":["### Finding your own dataset\n","Use the built-in datasets offered by scikit-learn to develop your own question to analyze using support vector machines. Follow the code below to learn how to import these datasets and display descriptive information to choose your favorite dataset. Use this time to get creative! "],"metadata":{"id":"yvLOwTfDbZuP"}},{"cell_type":"code","source":["from sklearn import datasets\n","\n","# List the available datasets:\n","dir(datasets) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ThFoJEIbyHX","executionInfo":{"status":"ok","timestamp":1682023957891,"user_tz":300,"elapsed":1722,"user":{"displayName":"Morgan Fogarty","userId":"08379757262064945343"}},"outputId":"255cb93b-1f9a-4b67-f7ea-e643bb834a8c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['__all__',\n"," '__builtins__',\n"," '__cached__',\n"," '__doc__',\n"," '__file__',\n"," '__getattr__',\n"," '__loader__',\n"," '__name__',\n"," '__package__',\n"," '__path__',\n"," '__spec__',\n"," '_arff_parser',\n"," '_base',\n"," '_california_housing',\n"," '_covtype',\n"," '_kddcup99',\n"," '_lfw',\n"," '_olivetti_faces',\n"," '_openml',\n"," '_rcv1',\n"," '_samples_generator',\n"," '_species_distributions',\n"," '_svmlight_format_fast',\n"," '_svmlight_format_io',\n"," '_twenty_newsgroups',\n"," 'clear_data_home',\n"," 'dump_svmlight_file',\n"," 'fetch_20newsgroups',\n"," 'fetch_20newsgroups_vectorized',\n"," 'fetch_california_housing',\n"," 'fetch_covtype',\n"," 'fetch_kddcup99',\n"," 'fetch_lfw_pairs',\n"," 'fetch_lfw_people',\n"," 'fetch_olivetti_faces',\n"," 'fetch_openml',\n"," 'fetch_rcv1',\n"," 'fetch_species_distributions',\n"," 'get_data_home',\n"," 'load_breast_cancer',\n"," 'load_diabetes',\n"," 'load_digits',\n"," 'load_files',\n"," 'load_iris',\n"," 'load_linnerud',\n"," 'load_sample_image',\n"," 'load_sample_images',\n"," 'load_svmlight_file',\n"," 'load_svmlight_files',\n"," 'load_wine',\n"," 'make_biclusters',\n"," 'make_blobs',\n"," 'make_checkerboard',\n"," 'make_circles',\n"," 'make_classification',\n"," 'make_friedman1',\n"," 'make_friedman2',\n"," 'make_friedman3',\n"," 'make_gaussian_quantiles',\n"," 'make_hastie_10_2',\n"," 'make_low_rank_matrix',\n"," 'make_moons',\n"," 'make_multilabel_classification',\n"," 'make_regression',\n"," 'make_s_curve',\n"," 'make_sparse_coded_signal',\n"," 'make_sparse_spd_matrix',\n"," 'make_sparse_uncorrelated',\n"," 'make_spd_matrix',\n"," 'make_swiss_roll',\n"," 'textwrap']"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["# Output a discription of the dataset \n","print(datasets.load_digits().DESCR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S9_RNesZcQcM","executionInfo":{"status":"ok","timestamp":1682024053780,"user_tz":300,"elapsed":102,"user":{"displayName":"Morgan Fogarty","userId":"08379757262064945343"}},"outputId":"40802ed7-95b0-4b45-f477-f7beb15e27f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".. _digits_dataset:\n","\n","Optical recognition of handwritten digits dataset\n","--------------------------------------------------\n","\n","**Data Set Characteristics:**\n","\n","    :Number of Instances: 1797\n","    :Number of Attributes: 64\n","    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n","    :Missing Attribute Values: None\n","    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n","    :Date: July; 1998\n","\n","This is a copy of the test set of the UCI ML hand-written digits datasets\n","https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n","\n","The data set contains images of hand-written digits: 10 classes where\n","each class refers to a digit.\n","\n","Preprocessing programs made available by NIST were used to extract\n","normalized bitmaps of handwritten digits from a preprinted form. From a\n","total of 43 people, 30 contributed to the training set and different 13\n","to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n","4x4 and the number of on pixels are counted in each block. This generates\n","an input matrix of 8x8 where each element is an integer in the range\n","0..16. This reduces dimensionality and gives invariance to small\n","distortions.\n","\n","For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n","T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n","L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n","1994.\n","\n",".. topic:: References\n","\n","  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n","    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n","    Graduate Studies in Science and Engineering, Bogazici University.\n","  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n","  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n","    Linear dimensionalityreduction using relevance weighted LDA. School of\n","    Electrical and Electronic Engineering Nanyang Technological University.\n","    2005.\n","  - Claudio Gentile. A New Approximate Maximal Margin Classification\n","    Algorithm. NIPS. 2000.\n","\n"]}]},{"cell_type":"markdown","source":["## **Apply PCA on your Dataset** \n","Let's start by loading your dataset and visualizing the data before performing dimensionality reduction. You can follow the steps provided in Part 3 of the instructions. However, keep in mind that there are various ways to visualize data, so feel free to explore different techniques and choose the approach that best suits your needs. This flexibility allows you to showcase the characteristics and patterns of your dataset effectively. Once you have visualized the data, you will gain insights into its structure and distribution, which will serve as a reference for comparison after dimensionality reduction."],"metadata":{"id":"Yfs6QY2h0ksf"}},{"cell_type":"code","source":["# Visualization of the data set\n","\n","############ Insert you rown code here ############  "],"metadata":{"id":"e6iSVBAD0sgD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Perform PCA on your own data set using the sklearn PCA function."],"metadata":{"id":"49_R3_430tbX"}},{"cell_type":"code","source":["# Data Prperation\n","\n","############ Insert you rown code here ############  \n","\n","# PCA\n","\n","############ Insert you rown code here ############  "],"metadata":{"id":"TtSjYS780tOo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualize your dimensionality reduced data. Compare your results to the initial dataset. "],"metadata":{"id":"atCuFeAI0s0b"}},{"cell_type":"code","source":["# Visualization of the dimensionality reduced data\n","\n","############ Insert you rown code here ############  "],"metadata":{"id":"mW-IgBSP1A1n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### <center> You are an absolute Pro - well done! </center>"],"metadata":{"id":"kpu__B9Y2bB8"}}]}