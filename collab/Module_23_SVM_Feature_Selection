{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["NivuMlhpsoOU","90tH2unUMA9r"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# <center><strong>Important:</strong> Make a Copy of this Google Colab Notebook!\n","</center>\n","\n","<p>Please refrain from using or modifying this current Google Colab notebook directly. Instead, follow these instructions to create your own copy:</p>\n","\n","<ol>\n","  <li>Go to the \"File\" menu at the top of the Colab interface.</li>\n","  <li>Select \"Save a copy in Drive\" to create a duplicate of this notebook.</li>\n","  <li>You can now work on your own copy without affecting the original.</li>\n","</ol>\n","\n","<p>This ensures that you have a personalized version to work on and make changes according to your needs. Remember to save your progress as you go. Enjoy working on your own copy of the Google Colab notebook!</p>"],"metadata":{"id":"M_xkV_Z9sRL2"}},{"cell_type":"markdown","source":["# **Module 23 Support Vector Machine Feature Selection**\n","In this module, you will use the same SVM algorithm from Module 21 but we will be including feature selection as a way to improve the model accuracy. For more information about these functions, please see Module 21.\n","\n","## **Getting Started**\n","Run the provided code sections and follow the instructions. Implement your own code were indicated."],"metadata":{"id":"tPWSySrGsgRV"}},{"cell_type":"markdown","source":["##**Importing Python Packages**\n","The first step is to import your necessary Python packages."],"metadata":{"id":"NivuMlhpsoOU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ke3AbLf8ukso"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from pandas.io.common import dataclasses\n","import time\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, recall_score, precision_score\n","from sklearn.utils import shuffle\n","from sklearn import datasets\n","\n","import statsmodels.api as sm  # for finding the p-value"]},{"cell_type":"markdown","source":["## **Functions from Module 21**"],"metadata":{"id":"90tH2unUMA9r"}},{"cell_type":"markdown","source":["First, let's copy over the `compute_cost`, `calculate_cost_gradient`, and `sgd` functions from Module 21. These functions are the builing blocks for our SVM algorithm and are defined below for usage later in this Colab notebook."],"metadata":{"id":"vDoRCwpRzYNC"}},{"cell_type":"code","source":["def compute_cost(W, X, Y):\n","    # calculate hinge loss\n","    N = X.shape[0]\n","    distances = 1 - Y * (np.dot(X, W))\n","    distances[distances < 0] = 0  # equivalent to max(0, distance)\n","    hinge_loss = reg_strength * (np.sum(distances) / N)\n","\n","    # calculate cost\n","    cost = 1 / 2 * np.dot(W, W) + hinge_loss\n","    return cost"],"metadata":{"id":"pcj20IVkUNYV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_cost_gradient(W, X_batch, Y_batch):\n","    Y_batch = np.array([Y_batch])\n","    X_batch = np.array([X_batch])\n","\n","    distance = 1 - (Y_batch * np.dot(X_batch, W))\n","    dw = np.zeros(len(W))\n","\n","    for ind, d in enumerate(distance):\n","        if max(0, d) == 0:\n","            di = W\n","        else:\n","            di = W - (reg_strength * Y_batch[ind] * X_batch[ind])\n","        dw += di\n","\n","    dw = dw/len(Y_batch)  # average\n","    return dw"],"metadata":{"id":"ZKSqCu67XSdk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sgd(features, outputs):\n","    weights = np.zeros(features.shape[1])\n","    # stochastic gradient descent\n","    for epoch in range(1, max_epochs):\n","        # shuffle to prevent repeating update cycles\n","        X, Y = shuffle(features, outputs)\n","        for ind, x in enumerate(X):\n","            ascent = calculate_cost_gradient(weights, x, Y[ind])\n","            weights = weights - (learning_rate * ascent)\n","\n","    return weights"],"metadata":{"id":"rmHp84AJaHg1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Stopping Criterion**"],"metadata":{"id":"_PUjtNlm0UjK"}},{"cell_type":"markdown","source":["In our original version of the SVM algorithm, we set the `max_epochs` to 5000. This means that our SVM algorithm will iterate 5000 times and then exit. Each of these iterations takes additional training time and computational resources. In this case, the additional time and resources may be minimal due to us using a small amount of data and a simple algorithm. When you are implementing machine learning techniques in your research or coursework, it can be beneficial to have a **stopping criterion** so the algorithm exits once the criterion is met.\n","\n","There are many varities of stopping criterion that can be implemented. In our example, we will use a simple one. Here, we will stop the training when the current cost has not decreased much as compared to the previous cost. This is implemented below by first computing the cost, and then compare the current cost to the previous cost. If this difference is less than a specified `cost_threshold`, then the algorithm will terminate."],"metadata":{"id":"IwLwzQeW0XdC"}},{"cell_type":"code","source":["def sgd_stop(features, outputs):\n","    weights = np.zeros(features.shape[1])\n","    prev_cost = float(\"inf\")\n","    # stochastic gradient descent\n","    for epoch in range(1, max_epochs):\n","        # shuffle to prevent repeating update cycles\n","        X, Y = shuffle(features, outputs)\n","        for ind, x in enumerate(X):\n","            ascent = calculate_cost_gradient(weights, x, Y[ind])\n","            weights = weights - (learning_rate * ascent)\n","            cost = compute_cost(weights, features, outputs)\n","            # stoppage criterion\n","            if abs(prev_cost - cost) < cost_threshold:\n","              print(\"The algorithm stopped at: \",epoch,\" epochs\")\n","              return weights\n","            prev_cost = cost\n","    return weights"],"metadata":{"id":"l1kKo3CV0X9K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Feature Selection**"],"metadata":{"id":"XFktHyqm0IRi"}},{"cell_type":"markdown","source":["Feature selection refers to the process of selecting a subset of features (or variables) from the original feature set to improve the performance of our model. Feature selection typically involves using statistical techniques to filter any of the redundant or irrelevant features. In this example, we will focus on using correlation and p-values to filter our features.\n","\n","Correlation is the degree of affine (linear + translation) dependence between two variables. Two features are correlated if the values of one feature can be explained by some affine relationship to the second feature. The degree of this relationship is given by the correlation coefficient (or “r”) which ranges from -1.0 to +1.0. The closer r is to +1 or -1, the more closely the two variables are related. We want to filter out any correlated features since correlated features will likely have the same effect on the model output. This means that correlated features will likely not improve the model, and in some cases would worsen performance. Fewer features also leads to improved training speeds as well as providing a simpler model.\n","\n","We will write our own function called `remove_correlated_features` in order to filter out any correlated features. This function iterates through the features and removes them if the correlation is greater than or equal to a defined `corr_threshold`. You can adjust this threshold when we set our parameters later in the script."],"metadata":{"id":"jEcFxU-U0MDq"}},{"cell_type":"code","source":["def remove_correlated_features(X):\n","    corr = X.corr()\n","    drop_columns = np.full(corr.shape[0], False, dtype=bool)\n","    for i in range(corr.shape[0]):\n","        for j in range(i + 1, corr.shape[0]):\n","            if corr.iloc[i, j] >= corr_threshold:\n","                drop_columns[j] = True\n","    columns_dropped = X.columns[drop_columns]\n","    X.drop(columns_dropped, axis=1, inplace=True)\n","    return columns_dropped"],"metadata":{"id":"-78sC-sb0NB6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Another method for feature selection involves using p-values. In the context of feature selection, p-values help us find the features which are most significant in explaining variation in the dependent variable (y). We can therefore remove the less significant features using a significance level (sl) threshold. We want to filter through out features and remove any that have large p-values, indicating that the feature is not significant. This is because these features don’t tell much about the behavior of the dependent variable and by keeping these features we are increasing the complexity of our model when they are not helping us in predicting the result.\n","\n","We can remove less significant features using the function below called `remove_less_significant_features`. This function computes the p-values across the columns and removes any columns with p-values that are greater than the significance level, `sl`, that we can set when defining our model parameters."],"metadata":{"id":"SpuJxN2J0zmC"}},{"cell_type":"code","source":["def remove_less_significant_features(X, Y):\n","    regression_ols = None\n","    columns_dropped = np.array([])\n","    for itr in range(0, len(X.columns)):\n","        regression_ols = sm.OLS(Y, X).fit()\n","        max_col = regression_ols.pvalues.idxmax()\n","        max_val = regression_ols.pvalues.max()\n","        if max_val > sl:\n","            X.drop(max_col, axis='columns', inplace=True)\n","            columns_dropped = np.append(columns_dropped, [max_col])\n","        else:\n","            break\n","    regression_ols.summary()\n","    return columns_dropped"],"metadata":{"id":"Nr2LYIwT0vwC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we have two functions that can be used for feature selection. These functions will be called prior to normalizing our data as you will see below in the \"Training our Model\" section of this tutorial."],"metadata":{"id":"s0knFVzUkdXZ"}},{"cell_type":"markdown","source":["## **Loading and Inspecting the Dataset**\n"],"metadata":{"id":"m6jzpIOLAj5R"}},{"cell_type":"markdown","source":["First, we will load the dataset from scikit-learn. This dataset contains information about breast cancer tumors from digitized images of fine needle aspirates (FNA) of breast masses. This is a commonly used dataset for binary classification tasks, where the objective is to predict whether a tumor is malignant (cancerous) or benign (non-cancerous).\n","\n","Within the dataset, there are 30 numerical features including the mean, standard error, and worst (largest) values of attributes such as radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, and fractal dimension.\n","\n","The target variable represents the diagnosis of the tumor and is encoded as follows:\n","*   0: Malignant (indicating the presence of cancer)\n","*   1: Benign (indicating the absence of cancer)"],"metadata":{"id":"xSZaX293IAaS"}},{"cell_type":"code","source":["cancer_data = datasets.load_breast_cancer()\n","data = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\n","\n","# Add the target variable to the DataFrame\n","data['diagnosis'] = cancer_data.target\n","diagnosis_map = {1:1, 0:-1} # Changing the values from 1:Malignant and 0:Benign to 1:Maligant and -1:Benign\n","data['diagnosis'] = data['diagnosis'].map(diagnosis_map)"],"metadata":{"id":"hpWOgaVBuaiG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we did previously, we will also normalize the data and split our data into testing and training datasets."],"metadata":{"id":"3RPhnlLLzzDs"}},{"cell_type":"code","source":["Y = data.loc[:, 'diagnosis']  # all rows of 'diagnosis', these are our targets\n","X = data.iloc[:, 1:30]  # all rows of column 1 and ahead, these are our features\n","\n","# normalize the features using MinMaxScalar from sklearn.preprocessing\n","X_normalized = MinMaxScaler().fit_transform(X.values)\n","X = pd.DataFrame(X_normalized)"],"metadata":{"id":"HMCPWCKdHY1q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# first insert 1 in every row for intercept b\n","X.insert(loc=len(X.columns), column='intercept', value=1)\n","\n","# test_size is the portion of data that will go into test set\n","# random_state is the seed used by the random number generator\n","X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"],"metadata":{"id":"OQHmMVEWI_gL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Training the Model**"],"metadata":{"id":"3l7PMAcvoNPY"}},{"cell_type":"markdown","source":["Now we can train the model using our SGD function. We will train three different models here.\n","\n","\n","1.   SVM without feature selection and stopping critera\n","2.   SVM with feature selection\n","3.   SVM with stopping critera\n","\n","In addition to investigating the differences in the accuracy, precision, and recall, we will also evaluate the time it takes to run each of these algorithms. To do this we will use `time` functions to compute the execution time for these three models."],"metadata":{"id":"t9RlyNG7oQHf"}},{"cell_type":"code","source":["reg_strength = 10000 # regularization strength\n","learning_rate = 0.000001 # step size\n","max_epochs = 5000 # number of times we do gradient descent\n","cost_threshold = 1e-6  # stopping critera\n","corr_threshold = 0.9\n","sl = 0.05"],"metadata":{"id":"_BnA82IFlcBY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Standard SVM algorithm\n","start_time = time.time()\n","W_svm = sgd(X_train.to_numpy(), y_train.to_numpy())\n","end_time = time.time()\n","execution_time_svm = end_time - start_time"],"metadata":{"id":"v43VgD4C8byz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SVM with stopping critera\n","start_time = time.time()\n","W_svm_stop = sgd_stop(X_train.to_numpy(), y_train.to_numpy())\n","end_time = time.time()\n","execution_time_svm_stop = end_time - start_time"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_6_TbNT8gGj","executionInfo":{"status":"ok","timestamp":1686846143721,"user_tz":300,"elapsed":17985,"user":{"displayName":"Morgan Fogarty","userId":"08379757262064945343"}},"outputId":"a3049703-55fb-4d04-9581-3a8d7796c48e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The algorithm stopped at:  694  epoch\n"]}]},{"cell_type":"code","source":["# SVM with feature selection\n","X_feat = pd.DataFrame(X)\n","Y_feat = pd.DataFrame(Y)\n","\n","remove_correlated_features(X_feat)\n","remove_less_significant_features(X_feat, Y_feat)\n","\n","X_feat_normalized = MinMaxScaler().fit_transform(X_feat.values)\n","X_feat = pd.DataFrame(X_feat_normalized)\n","X_feat.insert(loc=len(X_feat.columns), column='intercept', value=1)\n","\n","X_train_feat, X_test_feat, y_train_feat, y_test_feat = train_test_split(X_feat, Y_feat, test_size=0.2, random_state=42)\n","\n","start_time = time.time()\n","W_svm_feat = sgd(X_train_feat.to_numpy(), y_train_feat.to_numpy())\n","end_time = time.time()\n","execution_time_svm_feat = end_time - start_time"],"metadata":{"id":"z-G_Ysxm8k8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Standard SVM Execution time: \", execution_time_svm, \" seconds\")\n","print(\"SVM with Stopping Criteria Execution time: \", execution_time_svm_stop, \" seconds\")\n","print(\"SVM with Feature Selection Execution time: \", execution_time_svm_feat, \" seconds\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f8Qe64YD8oRr","executionInfo":{"status":"ok","timestamp":1686846181999,"user_tz":300,"elapsed":13,"user":{"displayName":"Morgan Fogarty","userId":"08379757262064945343"}},"outputId":"c92a4b00-cc7e-4f40-8de9-2ac0ebc244a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Standard SVM Execution time:  37.99506664276123  seconds\n","SVM with Stopping Criteria Execution time:  17.921315908432007  seconds\n","SVM with Feature Selection Execution time:  38.26032614707947  seconds\n"]}]},{"cell_type":"markdown","source":["❓ **What do you notice about the training time for these four models?** ❓\n","\n","Discuss on Discord or Piazza the differences you see between the training time for SVM with and without the stopping critiera. What do you notice about the training time between the standard SVM algorithm and when we include feature selection? Why do you think these changes in training time are happening?"],"metadata":{"id":"tXOXkmLkARur"}},{"cell_type":"markdown","source":["## **Testing the Model**"],"metadata":{"id":"W9CFbaPaoHex"}},{"cell_type":"markdown","source":["Now we can test the model using the weights from training the model. Here we can compare the results with and without the addition of feature selection and a stopping critera.\n"],"metadata":{"id":"ppC_0k01oKXZ"}},{"cell_type":"code","source":["y_test_predicted_svm = np.array([])\n","y_test_predicted_svm_stop = np.array([])\n","y_test_predicted_svm_feat = np.array([])\n","y_test_predicted_svm_feat_stop = np.array([])\n","\n","for i in range(X_test.shape[0]):\n","    yp_svm = np.sign(np.dot(X_test.to_numpy()[i], W_svm))\n","    y_test_predicted_svm = np.append(y_test_predicted_svm, yp_svm)\n","\n","    yp_svm_stop = np.sign(np.dot(X_test.to_numpy()[i], W_svm_stop))\n","    y_test_predicted_svm_stop = np.append(y_test_predicted_svm_stop, yp_svm_stop)\n","\n","    yp_svm_feat = np.sign(np.dot(X_test_feat.to_numpy()[i], W_svm_feat))\n","    y_test_predicted_svm_feat = np.append(y_test_predicted_svm_feat, yp_svm_feat)\n","\n","print(\"Accuracy using SVM: {}\".format(accuracy_score(y_test, y_test_predicted_svm)))\n","print(\"Recall using SVM: {}\".format(recall_score(y_test, y_test_predicted_svm)))\n","print(\"Precision using SVM: {}\".format(precision_score(y_test, y_test_predicted_svm)))\n","print(\"\")\n","\n","print(\"Accuracy using SVM with stopping critera: {}\".format(accuracy_score(y_test, y_test_predicted_svm_stop)))\n","print(\"Recall using SVM with stopping critera: {}\".format(recall_score(y_test, y_test_predicted_svm_stop)))\n","print(\"Precision using SVM with stopping critera: {}\".format(precision_score(y_test, y_test_predicted_svm_stop)))\n","print(\"\")\n","\n","print(\"Accuracy using SVM with feature selection: {}\".format(accuracy_score(y_test_feat, y_test_predicted_svm_feat)))\n","print(\"Recall using SVM with feature selection: {}\".format(recall_score(y_test_feat, y_test_predicted_svm_feat)))\n","print(\"Precision using SVM with feature selection: {}\".format(precision_score(y_test_feat, y_test_predicted_svm_feat)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9J36MsWelinP","executionInfo":{"status":"ok","timestamp":1686846182125,"user_tz":300,"elapsed":137,"user":{"displayName":"Morgan Fogarty","userId":"08379757262064945343"}},"outputId":"330d1e5b-a669-4cb2-e2fd-ad81bbc0fde0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy using SVM: 0.9736842105263158\n","Recall using SVM: 0.9859154929577465\n","Precision using SVM: 0.9722222222222222\n","\n","Accuracy using SVM with stopping critera: 0.9824561403508771\n","Recall using SVM with stopping critera: 1.0\n","Precision using SVM with stopping critera: 0.9726027397260274\n","\n","Accuracy using SVM with feature selection: 0.9912280701754386\n","Recall using SVM with feature selection: 1.0\n","Precision using SVM with feature selection: 0.9861111111111112\n","\n"]}]},{"cell_type":"markdown","source":["❓ **What do you notice about the accuracy metrics for these four models?** ❓\n","\n","Discuss on Discord or Piazza the differences you see between the accuracy, precision, and recall between the SVM models with and without the feature selection? Do you notice any differences between the models with and without the stopping critera? Why do you think you notice or don't notice a difference?"],"metadata":{"id":"bBWx9SS6BMFk"}},{"cell_type":"markdown","source":["## **Adjusting SVM Parameters**"],"metadata":{"id":"2msukAr-Q4yY"}},{"cell_type":"markdown","source":["Now let's make some adjustments to the parameters `max_epochs`, `cost_threshold`, `corr_threshold`, and `sl`. Change these values and note the changes in the training time, accuracy, recall, and precision.\n","\n","*   What happens to the training time when you increase the `max_epochs`? How does this change the difference in training time between the SVM model with and without the stopping critera?\n","*   What happens to the accuracy and training time when you increase or decrease the `cost_threshold`?\n","*   What happens to the accuracy when you increase or decrease the `corr_threshold`?\n","*   What happens to the accuracy when you set the significance level, `sl`, to `0.001` or `0.01`?\n","\n","✅ **Discuss the changes in training time, accuracy, precision, and recall on Piazza or Discord with your fellow classmates!**\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"3UWPZxi3PvQ5"}},{"cell_type":"markdown","source":["## ✨ **Congratulations you have now implemented feature selection within the SVM algorithm using Python!** ✨"],"metadata":{"id":"gNn3BPyAp47A"}},{"cell_type":"markdown","source":["This code is from https://towardsdatascience.com/svm-implementation-from-scratch-python-2db2fc52e5c2#7944 which provides additional information on coding the SVM algorithm from scratch."],"metadata":{"id":"ZJo5ixEpqq6Z"}}]}