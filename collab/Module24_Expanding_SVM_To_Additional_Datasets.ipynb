{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["mmbLG6jtWa2p"],"authorship_tag":"ABX9TyPZ9129+5m1dGzOB3dKQulM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# <center><strong>Important:</strong> Make a Copy of this Google Colab Notebook!\n","</center>\n","\n","<p>Please refrain from using or modifying this current Google Colab notebook directly. Instead, follow these instructions to create your own copy:</p>\n","\n","<ol>\n","  <li>Go to the \"File\" menu at the top of the Colab interface.</li>\n","  <li>Select \"Save a copy in Drive\" to create a duplicate of this notebook.</li>\n","  <li>You can now work on your own copy without affecting the original.</li>\n","</ol>\n","\n","<p>This ensures that you have a personalized version to work on and make changes according to your needs. Remember to save your progress as you go. Enjoy working on your own copy of the Google Colab notebook!</p>"],"metadata":{"id":"Qv3KQsm9Tifo"}},{"cell_type":"markdown","source":["# **Module 24 Expanding SVM to Additional Datasets**\n","In this module, you will apply Python built-in SVM functions for a binary classfication task from scikit-learn. The goal is for you to implement these functions on a dataset of your choosing from scikit-learn datasets. You can use Module 22 for guidance.\n","\n","## **Getting Started**\n","\n","Run the provided code sections and follow the instructions."],"metadata":{"id":"TYBUj6p7TmCP"}},{"cell_type":"markdown","source":["##**Importing Python Packages**\n","The first step is to import your necessary Python packages."],"metadata":{"id":"gsTaczYew0EV"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"l157Vfvnu2jn","executionInfo":{"status":"ok","timestamp":1686853176596,"user_tz":300,"elapsed":2266,"user":{"displayName":"Morgan Fogarty","userId":"08379757262064945343"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn import svm\n","from sklearn import metrics\n","from sklearn.inspection import DecisionBoundaryDisplay"]},{"cell_type":"markdown","source":["## **Choosing a New Dataset**\n","\n"],"metadata":{"id":"ONj3cxD6CgB4"}},{"cell_type":"markdown","source":["There are many datasets available that can be used for the task of binary classification. In this example, we will continue to use datasets made readily available to use through scikit-learn. Let's start by exploring the datasets that are available within scikit-learn. To do this, we'll start by listing all of the availabe datasets using the `dir` command. This function lists all of the items within a given directory. In this case, our directory is `datasets` since we imported the datasets from sklearn above."],"metadata":{"id":"1HOPDhmkYXBz"}},{"cell_type":"code","source":["dir(datasets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n5zUOe5lSKqL","executionInfo":{"status":"ok","timestamp":1686853177679,"user_tz":300,"elapsed":315,"user":{"displayName":"Morgan Fogarty","userId":"08379757262064945343"}},"outputId":"6c1570e5-bd03-4304-9947-3bac3fead022"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['__all__',\n"," '__builtins__',\n"," '__cached__',\n"," '__doc__',\n"," '__file__',\n"," '__getattr__',\n"," '__loader__',\n"," '__name__',\n"," '__package__',\n"," '__path__',\n"," '__spec__',\n"," '_arff_parser',\n"," '_base',\n"," '_california_housing',\n"," '_covtype',\n"," '_kddcup99',\n"," '_lfw',\n"," '_olivetti_faces',\n"," '_openml',\n"," '_rcv1',\n"," '_samples_generator',\n"," '_species_distributions',\n"," '_svmlight_format_fast',\n"," '_svmlight_format_io',\n"," '_twenty_newsgroups',\n"," 'clear_data_home',\n"," 'dump_svmlight_file',\n"," 'fetch_20newsgroups',\n"," 'fetch_20newsgroups_vectorized',\n"," 'fetch_california_housing',\n"," 'fetch_covtype',\n"," 'fetch_kddcup99',\n"," 'fetch_lfw_pairs',\n"," 'fetch_lfw_people',\n"," 'fetch_olivetti_faces',\n"," 'fetch_openml',\n"," 'fetch_rcv1',\n"," 'fetch_species_distributions',\n"," 'get_data_home',\n"," 'load_breast_cancer',\n"," 'load_diabetes',\n"," 'load_digits',\n"," 'load_files',\n"," 'load_iris',\n"," 'load_linnerud',\n"," 'load_sample_image',\n"," 'load_sample_images',\n"," 'load_svmlight_file',\n"," 'load_svmlight_files',\n"," 'load_wine',\n"," 'make_biclusters',\n"," 'make_blobs',\n"," 'make_checkerboard',\n"," 'make_circles',\n"," 'make_classification',\n"," 'make_friedman1',\n"," 'make_friedman2',\n"," 'make_friedman3',\n"," 'make_gaussian_quantiles',\n"," 'make_hastie_10_2',\n"," 'make_low_rank_matrix',\n"," 'make_moons',\n"," 'make_multilabel_classification',\n"," 'make_regression',\n"," 'make_s_curve',\n"," 'make_sparse_coded_signal',\n"," 'make_sparse_spd_matrix',\n"," 'make_sparse_uncorrelated',\n"," 'make_spd_matrix',\n"," 'make_swiss_roll',\n"," 'textwrap']"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["As we can see there are many datasets that are available from scikit-learn. However, only some of these datasets are useful for simple binary classification tasks. You can learn more about the contents of these datasets to determine if they are suitable for binary classification by outputting a description of the dataset. We can do this by printing the `.DESCR` for a given dataset. In the example below, we output the description for the `load_breast_cancer` dataset that we used for the previous modules."],"metadata":{"id":"fSniFanmSmNC"}},{"cell_type":"code","source":["print(datasets.load_breast_cancer().DESCR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nFA3kgR1S5fr","executionInfo":{"status":"ok","timestamp":1686853770175,"user_tz":300,"elapsed":258,"user":{"displayName":"Morgan Fogarty","userId":"08379757262064945343"}},"outputId":"c615bbc7-6f71-4e32-e8e3-6ec54819200f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":[".. _breast_cancer_dataset:\n","\n","Breast cancer wisconsin (diagnostic) dataset\n","--------------------------------------------\n","\n","**Data Set Characteristics:**\n","\n","    :Number of Instances: 569\n","\n","    :Number of Attributes: 30 numeric, predictive attributes and the class\n","\n","    :Attribute Information:\n","        - radius (mean of distances from center to points on the perimeter)\n","        - texture (standard deviation of gray-scale values)\n","        - perimeter\n","        - area\n","        - smoothness (local variation in radius lengths)\n","        - compactness (perimeter^2 / area - 1.0)\n","        - concavity (severity of concave portions of the contour)\n","        - concave points (number of concave portions of the contour)\n","        - symmetry\n","        - fractal dimension (\"coastline approximation\" - 1)\n","\n","        The mean, standard error, and \"worst\" or largest (mean of the three\n","        worst/largest values) of these features were computed for each image,\n","        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n","        10 is Radius SE, field 20 is Worst Radius.\n","\n","        - class:\n","                - WDBC-Malignant\n","                - WDBC-Benign\n","\n","    :Summary Statistics:\n","\n","    ===================================== ====== ======\n","                                           Min    Max\n","    ===================================== ====== ======\n","    radius (mean):                        6.981  28.11\n","    texture (mean):                       9.71   39.28\n","    perimeter (mean):                     43.79  188.5\n","    area (mean):                          143.5  2501.0\n","    smoothness (mean):                    0.053  0.163\n","    compactness (mean):                   0.019  0.345\n","    concavity (mean):                     0.0    0.427\n","    concave points (mean):                0.0    0.201\n","    symmetry (mean):                      0.106  0.304\n","    fractal dimension (mean):             0.05   0.097\n","    radius (standard error):              0.112  2.873\n","    texture (standard error):             0.36   4.885\n","    perimeter (standard error):           0.757  21.98\n","    area (standard error):                6.802  542.2\n","    smoothness (standard error):          0.002  0.031\n","    compactness (standard error):         0.002  0.135\n","    concavity (standard error):           0.0    0.396\n","    concave points (standard error):      0.0    0.053\n","    symmetry (standard error):            0.008  0.079\n","    fractal dimension (standard error):   0.001  0.03\n","    radius (worst):                       7.93   36.04\n","    texture (worst):                      12.02  49.54\n","    perimeter (worst):                    50.41  251.2\n","    area (worst):                         185.2  4254.0\n","    smoothness (worst):                   0.071  0.223\n","    compactness (worst):                  0.027  1.058\n","    concavity (worst):                    0.0    1.252\n","    concave points (worst):               0.0    0.291\n","    symmetry (worst):                     0.156  0.664\n","    fractal dimension (worst):            0.055  0.208\n","    ===================================== ====== ======\n","\n","    :Missing Attribute Values: None\n","\n","    :Class Distribution: 212 - Malignant, 357 - Benign\n","\n","    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n","\n","    :Donor: Nick Street\n","\n","    :Date: November, 1995\n","\n","This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n","https://goo.gl/U2Uwz2\n","\n","Features are computed from a digitized image of a fine needle\n","aspirate (FNA) of a breast mass.  They describe\n","characteristics of the cell nuclei present in the image.\n","\n","Separating plane described above was obtained using\n","Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n","Construction Via Linear Programming.\" Proceedings of the 4th\n","Midwest Artificial Intelligence and Cognitive Science Society,\n","pp. 97-101, 1992], a classification method which uses linear\n","programming to construct a decision tree.  Relevant features\n","were selected using an exhaustive search in the space of 1-4\n","features and 1-3 separating planes.\n","\n","The actual linear program used to obtain the separating plane\n","in the 3-dimensional space is that described in:\n","[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n","Programming Discrimination of Two Linearly Inseparable Sets\",\n","Optimization Methods and Software 1, 1992, 23-34].\n","\n","This database is also available through the UW CS ftp server:\n","\n","ftp ftp.cs.wisc.edu\n","cd math-prog/cpo-dataset/machine-learn/WDBC/\n","\n",".. topic:: References\n","\n","   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n","     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n","     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n","     San Jose, CA, 1993.\n","   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n","     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n","     July-August 1995.\n","   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n","     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n","     163-171.\n"]}]},{"cell_type":"markdown","source":["For the task of binary classification, we recommend the following datasets which can be modified to work for a binary classification task:\n","\n","*   `load_wine`\n","*   `load_iris`\n","*   `load_digits`\n","*   `load_diabetes`\n","\n","While several of these datasets contain more than 2 classes, you can choose to only use data from 2 of the classes in order to have a dataset for binary classification. This can be done by augmenting the dataset to remove any rows that contain data from the other classes which you are not considering in your SVM model. We recommend using the `np.where` function to get a list of index values for the rows you wish to keep from the data. This can be done using `indices_to_keep = np.where(y != classToRemove)[0]` where `classToRemove` is equal to which class you with to filter from the dataset. You can then filter your feature matrix and target vector using `X_filtered = X[indices_to_keep]` where `X` is the matrix or vector you are trying to filter.\n","\n","Load and (if necessary) augment your dataset below:\n"],"metadata":{"id":"ty5YgWYuUfAM"}},{"cell_type":"code","source":["# Load dataset\n","\n","# Make adjustment for binary classification if necessary:\n"],"metadata":{"id":"_YDtXf2kVNoj","executionInfo":{"status":"ok","timestamp":1686854409900,"user_tz":300,"elapsed":110,"user":{"displayName":"Morgan Fogarty","userId":"08379757262064945343"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["## **Implementing SVM Using a New Dataset**"],"metadata":{"id":"mmbLG6jtWa2p"}},{"cell_type":"markdown","source":["### **Splitting the Dataset**"],"metadata":{"id":"fGUXCl3YbphW"}},{"cell_type":"markdown","source":["Next, we need to split the dataset into our training and testing datasets. This is necessary since we need to retain a testing dataset to see how the model will perform on unseen observations. To split the dataset, we'll use the function `train_test_split()` from sklearn.model_selection. The function takes in the following:\n","*   `X`: The feature data, typically represented as a NumPy array or pandas DataFrame.\n","*   `y`: The target variable or label data, corresponding to the feature data.\n","*   `test_size`: The proportion (between 0.0 and 1.0) of the dataset to include in the test split. For example, `test_size=0.3` would create a test set comprising 30% of the total data, while the remaining 70% is allocated to the training set.\n","*   `random_state`: The seed value used for random shuffling and splitting of the dataset. Setting a specific random_state ensures reproducibility of the split. If random_state is not provided, the data will be split differently each time the function is called.\n","\n","The `train_test_split()` function returns four subsets:\n","*   `X_train`: The training set of feature data.\n","*   `X_test`: The test set of feature data.\n","*   `y_train`: The corresponding target variable for the training set.\n","*   `y_test`: The corresponding target variable for the test set."],"metadata":{"id":"HvAVd1zvbx6V"}},{"cell_type":"code","source":["# Implement code for using train_test_split here:\n"],"metadata":{"id":"QSfY9YgFb4bK","executionInfo":{"status":"ok","timestamp":1686854252696,"user_tz":300,"elapsed":217,"user":{"displayName":"Morgan Fogarty","userId":"08379757262064945343"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["### **Training the Model**"],"metadata":{"id":"o5lHk_Xzb-jU"}},{"cell_type":"markdown","source":["Now that we have our training and testing datasets, we can run the SVM algorithm using the bulit in functions from scikit-learn. The model we are using is the `svm.SVC` function where SVC stands for support vector classification. The inputs to the fuction that we will focus on are `kernel` and `C`.\n","\n","*   kernel: Specifies the kernel type to be used in the SVM algorithm. Common choices include 'linear', 'poly', 'rbf' (Radial Basis Function), 'sigmoid', and more. The default is 'rbf'.\n","*   C: Penalty parameter C of the error term. It controls the trade-off between maximizing the margin and minimizing the classification errors. Higher values of C prioritize correct classification of training examples, potentially leading to overfitting. The default value is 1.0.\n","\n","We are going to focus on using a linear kernel since we want to linearly separate the data. However, we can adjust the value for the regularization strength, `C`.\n","\n","We will also be using the function `svm.SVC.fit` which trains the SVM model on the given training data `X_train` and corresponding labels `y_train`. Since we set the output of `svm.SVC` to be `model`, we can call this function as `model.fit`.\n"],"metadata":{"id":"h8WZDagJobsN"}},{"cell_type":"code","source":["# Write code to use svm.SVC and svm.SVC.fit here:\n"],"metadata":{"id":"PJ0ZwskHcBXE","executionInfo":{"status":"ok","timestamp":1686854254707,"user_tz":300,"elapsed":279,"user":{"displayName":"Morgan Fogarty","userId":"08379757262064945343"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["### **Testing the Model**"],"metadata":{"id":"mkfuL1hPcBuM"}},{"cell_type":"markdown","source":["Now we can use the SVM model in order to predict the labels on the testing dataset. This is done using the `svm.SVC.predict` function which we are calling as `model.predict`. The input to this function is the testing data and the output is the predicted labels from the SVM algorithm.\n","\n","Additionally, we can compute three metrics to describe the results of the prediction. In this case, we are using the accuracy, precision, and recall.\n","\n","*   **Precision**: Precision is a measure of the model's ability to correctly identify positive instances (true positives) out of all instances predicted as positive. It is calculated as the ratio of true positives (TP) to the sum of true positives and false positives (FP).\n","*   **Recall (Sensitivity or True Positive Rate)**: Recall measures the ability of the model to correctly identify positive instances (true positives) out of all actual positive instances. It is calculated as the ratio of true positives (TP) to the sum of true positives and false negatives (FN).\n","*   **Accuracy**: Accuracy measures the overall correctness of the model's predictions. It calculates the ratio of the number of correct predictions (true positives and true negatives) to the total number of instances. Accuracy provides an overall measure of the model's performance, considering both positive and negative predictions. However, accuracy alone may not be sufficient if the dataset is imbalanced (i.e., when the number of instances in one class is much higher than the other), as the model may achieve high accuracy by simply predicting the majority class.\n","\n","\n","\n"],"metadata":{"id":"vXpbIR5HspRf"}},{"cell_type":"code","source":["# Write code to use svm.SVC.predict here:\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VGnh3bzRcDbE","executionInfo":{"status":"ok","timestamp":1686854256588,"user_tz":300,"elapsed":134,"user":{"displayName":"Morgan Fogarty","userId":"08379757262064945343"}},"outputId":"0bf608f3-b51b-47de-e5cc-ba439d00bfe8"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on test dataset: 0.9230769230769231\n","Precision on test dataset: 1.0\n","Recall on test dataset: 0.8421052631578947\n"]}]},{"cell_type":"code","source":["# Write code to output the accuracy, precision, and recall here using metrics.accuracy_score, metrics.precision_score, and metrics.recall_score:\n","\n","Accuracy =\n","Precision =\n","Recall =\n","\n","print(\"Accuracy on test dataset:\",Accuracy)\n","print(\"Precision on test dataset:\",Precision)\n","print(\"Recall on test dataset:\",Recall)"],"metadata":{"id":"q0laWlOHn9Ro"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ✨ **Congratulations you have now coded the SVM algorithm using scikit-learn!** ✨"],"metadata":{"id":"8FpZJNZnvxq2"}}]}