{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# <center><strong>Important:</strong> Make a Copy of this Google Colab Notebook!\n","</center>\n","\n","<p>Please refrain from using or modifying this current Google Colab notebook directly. Instead, follow these instructions to create your own copy:</p>\n","\n","<ol>\n","  <li>Go to the \"File\" menu at the top of the Colab interface.</li>\n","  <li>Select \"Save a copy in Drive\" to create a duplicate of this notebook.</li>\n","  <li>You can now work on your own copy without affecting the original.</li>\n","</ol>\n","\n","<p>This ensures that you have a personalized version to work on and make changes according to your needs. Remember to save your progress as you go. Enjoy working on your own copy of the Google Colab notebook!</p>\n"],"metadata":{"id":"Uw8Y_S4LSEeP"}},{"cell_type":"markdown","source":["# **Module 12 Principal Component Analysis for Dimensionality Reduction**\n","In this module, you will apply your knowledge from the past couple of modules to implement PCA using Python. The goal is to write the algorithm by yourself from scratch using Python. Next, you will utilize your PCA function to analyze the principal components of a real-life data set. <p>\n","\n","This practice is divided into 4 Parts, with the last being optional. <p>\n","\n","<ul><li> Part 1: Step-by-Step Guide for Coding Principal Component Analysis </li>\n","<li> Part 2: Write Your Own PCA Function </li>\n","<li> Part 3: Apply your PCA Function on the IRIS dataset </li>\n","<li> [OPTIONAL] Part 4: Apply your PCA Function on Your Own Dataset </li> </ul>\n","\n","<i>Note: Depending on your previous experience with Python and familiarity with PCA, Parts 1-3 may feel relatively straightforward. As a result, we have included an optional Part 4 to provide an additional challenge. If you are new to these concepts or prefer to focus on Parts 1-3, skip Part 4.Â  </i> <p>\n","\n","## **Getting Started**\n","\n","Run the provided code sections and follow the instructions. Implement your own code were indicated."],"metadata":{"id":"IPqsidvrgbCy"}},{"cell_type":"markdown","source":["## **Importing Python Packages**\n","The first step is to import your necessary Python packages. For this example, we'll be using numpy to implement prinicpal component analysis."],"metadata":{"id":"C07k1MskpxQ4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKFSar5XgHy3"},"outputs":[],"source":["import numpy as np # Importing the numpy library as np - this is a common practice for Python coding\n","import matplotlib.pyplot as plt # Importing the matplotlib.pyplot as plt - useful for visualizing data"]},{"cell_type":"markdown","source":["## **Generate a Dummy Dataset**\n","\n","Before applying your algorithm to a dataset, we will generate dummy data to get you started. Generating dummy data is common practice in Machine Learning classes and can be very useful for analyizing or building algorithms to analyze data. <p>\n","We will also show you how to visualize the data and investigate the shape of the data. These are all helpful tool that help you to get a better understanding of the data you are working with."],"metadata":{"id":"6RKEz7kmq5sD"}},{"cell_type":"code","source":["# Generate data points\n","np.random.seed(10) # pseudo-random number generation, we will get the same set of random numbers every time\n","X = np.random.randint(10, 50, 100).reshape(20, 5)  # Randomly generate a 20x5 matrix of integers between 10 and 50\n","\n","# Check the size of the data\n","data_shape = X.shape\n","print(\"Size of the data:\", data_shape)"],"metadata":{"id":"d9ozCxHEyFDH","executionInfo":{"status":"error","timestamp":1689721318416,"user_tz":300,"elapsed":211,"user":{"displayName":"Mahshid Naghashzadeh","userId":"03100905172867360472"}},"outputId":"be6b04e6-e297-4344-f47e-47dbd47f23a2","colab":{"base_uri":"https://localhost:8080/","height":232}},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-9312fd756a6f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate data points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# pseudo-random number generation, we will get the same set of random numbers every time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Randomly generate a 20x5 matrix of integers between 10 and 50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Check the size of the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}]},{"cell_type":"code","source":["# Plotting the data\n","plt.figure(figsize=(4, 3))\n","plt.scatter(X[:, 0], X[:, 1], c='blue', label='Data Points')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.title('Visualization Data Points')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"ZtoedFLYafUU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The data generated from the previous code snippet has a shape of (20, 5), meaning it consists of 20 examples with 5 variables for each example. To normalize the data, we computed the mean value for each variable and then subtracted that mean from every value in the respective column."],"metadata":{"id":"0VMgqHm0b4af"}},{"cell_type":"markdown","source":["## **Part 1: Step-by-Step Guide for Coding Principal Component Analysis**\n","In this section, we will guide you through the implementation steps of Principal Component Analysis (PCA). We highly recommend that you try to complete the code by yourself, utilizing the NumPy package. It will be beneficial to explore the [documentation](https://numpy.org/doc/) and familiarize yourself with the available functions within NumPy. We encourage you to refrain from relying on external sources on the internet for this exercise. By attempting the implementation independently, you can strengthen your understanding of PCA and enhance your problem-solving skills. Let's begin the journey!\n","\n","<ul><li> <strong>NumPy Basics</strong>: <a href=\"url\"> https://numpy.org/doc/stable/user/absolute_beginners.html </a>\n","<li> <strong>NumPy User Guide</strong>: <a href=\"url\"> https://numpy.org/doc/1.23/numpy-user.pdf </a></ul>\n","\n","\n","### **Standardizing the Data**\n","PCA requires the data to be standardized (mean = 0, variance =1). <p>\n","\n","Here are some articles on why this is important:\n","<ul><li> <strong>Scikit Learn</strong>: <a href=\"url\"> https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html</a>\n","<li> <strong>Blog Post</strong>: <a href=\"url\"> https://builtin.com/data-science/when-and-why-standardize-your-data </a></ul>\n","\n","<i>Feel free to compare your PCA results with and without standardized data. </i>\n"],"metadata":{"id":"GTSEY7qhdCiy"}},{"cell_type":"code","source":["# Standardized data  (mean = 0, variance = 1)\n","print('Mean: ', np.mean(X, axis=0),'Variance: ', np.std(X, axis=0)) #show variance and mean before normalizing\n","\n","X_centered =\n","X_norm =\n","\n","print('Mean: ', np.mean(X_norm, axis=0), 'Variance: ', np.std(X_norm, axis=0)) #show variance and mean after normalizing\n","\n","# Visualize the mean centered data\n","plt.figure(figsize=(4, 3))\n","plt.scatter(X_centered[:, 0], X_norm[:, 1], c='blue', label='Data Points')\n","plt.xlabel('Standardized Feature 1')\n","plt.ylabel('Standardized Feature 2')\n","plt.title('Visualization Data Points')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"_cwJlrV_d8cs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","<p> We provided you with the code for mean centering the data, to get you started. Now, you need to fill in code by yourself. </p>\n","\n","<p><i> Note, there are many different ways of how to implement the different steps. However, each step should not require you to write more than 1 to 4 lines. </i> </p>\n"],"metadata":{"id":"CNpYGYBrfsEs"}},{"cell_type":"code","source":["# Calculating the covariance matrix of the mean-centered data.\n","cov_mat = ############ Insert your own code here ############\n","\n","# Calculating Eigenvalues and Eigenvectors of the covariance matrix\n","eigen_values, eigen_vectors = ############ Insert you own code here ############\n","\n","# Sorting Eigenvalues and Eigenvectors\n","# hint: create a variable for storing indeces when sorting\n","sorted_eigenvalues = ############ Insert your own code here ############\n","sorted_eigenvectors = ############ Insert your own  code here ############\n","\n","# Calculate Total Variance Explained\n","explained_variance_1 = ############ Insert your own  code here ############\n","\n","# Select a subset of principal components\n","n_components = 2 # you can select any number of components.\n","eigenvector_subset = ############ Insert your own  code here ############\n","\n","# Transform the data\n","X_reduced_1 = ############ Insert your own  code here ############\n","\n","# Print explained Variance for selected Principal Components\n","############ Insert your own  code here ############\n"],"metadata":{"id":"o-IFRQv2ghPg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Look at the results."],"metadata":{"id":"HRaNpYcqth3w"}},{"cell_type":"code","source":["# Print explained variance ratios\n","for i, exp_var in enumerate(explained_variance_1[:n_components]):\n","    print(f\"Explained Variance for PC{i+1}: {exp_var*100:.2f}%\")\n","\n","# Check the size of the data\n","print(\"\\nSize of the original data: %s\\nSize of the dimensionality reduced data: %s\" % (X.shape, X_reduced_1.shape))"],"metadata":{"id":"tgVJYAU_tkVM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Part 2: Write Your Own PCA Function**\n","Now we will combine all steps into a function. A function allows you to break down your program into smaller, modular units, making your code more organized and readable. Functions take input values, process them, and optionally return results. They promote code reusability, allowing you to use the same function multiple times in different parts of your program. Functions enhance code readability by abstracting complex logic into simpler, self-contained units. They also facilitate code maintenance, as changes or updates can be made in one place, reflecting across all function calls. <p>\n","\n","Fill in the blanks using the code snippets from Part 1."],"metadata":{"id":"dPkAm16ofMf_"}},{"cell_type":"code","source":["def PCA_custom(X, nComponents):\n","\n","  # normalize data\n","  X_centered = ############ Insert your own  code here ############\n","  X_norm = ############ Insert your own code here ############\n","\n","  # Calculating the covariance matrix of the mean-centered data.\n","  cov_mat = ############ Insert your own code here ############\n","\n","  # Calculating Eigenvalues and Eigenvectors of the covariance matrix\n","  eigen_values, eigen_vectors = ############ Insert your own code here ############\n","\n","  # Sorting Eigenvalues and Eigenvectors\n","  sorted_eigenvalues = ############ Insert your own code here ############\n","  sorted_eigenvectors = ############ Insert your own code here ############\n","\n","  # Calculate Total Variance Explained\n","  explained_variance = ############ Insert your own code here ############\n","\n","  # Select a subset of principal components\n","  eigenvector_subset = ############ Insert your own code here ############\n","\n","  # Transform the data\n","  X_reduced = ############ Insert your own code here ############\n","\n","  return X_reduced, explained_variance"],"metadata":{"id":"XZ9JfX00motx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's compare the results from Part 1 and Part 2. Make sure both results are the same before moving on."],"metadata":{"id":"bo4H3c4_tI79"}},{"cell_type":"code","source":["# Print explained variance ratios\n","for i, exp_var in enumerate(explained_variance_1[:n_components]):\n","    print(f\"Explained Variance for PC{i+1}: {exp_var*100:.2f}%\")\n","\n","# Check the size of the data\n","print(\"\\nSize of the original data: %s\\nSize of the dimensionality reduced data: %s\" % (X.shape, X_reduced_1.shape))\n","\n","# result Part 2\n","[X_reduced_2, explained_variance_2] = PCA_custom(X,2)\n","\n","# Print explained variance ratios\n","for i, exp_var in enumerate(explained_variance_2[:n_components]):\n","    print(f\"Explained Variance for PC{i+1}: {exp_var*100:.2f}%\")\n","\n","# Check the size of the data\n","print(\"\\nSize of the original data: %s\\nSize of the dimensionality reduced data: %s\" % (X.shape, X_reduced_2.shape))"],"metadata":{"id":"reWOmPKCtOMM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Well done, let's call the function we just created to perform PCA on a real life data set in Part 3.\n"],"metadata":{"id":"JsAvkG7gpOPO"}},{"cell_type":"markdown","source":["## **Part 3: Apply your PCA Function on the [IRIS datset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)**\n","\n","The IRIS dataset consists of 3 different types of irisesâ (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray <p>\n","\n","The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.<p>\n","\n","The below plot uses the first two features. See here for more information on this dataset.<p>\n","\n","<i>Source: The description of the datatset is copied from the [scikit-learn webpage](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html). </i><p>\n"],"metadata":{"id":"ntqzzBBFs4Xe"}},{"cell_type":"code","source":["import pandas as pd # libary for data management\n","import seaborn as sb # library for visualization\n","import matplotlib.pyplot as plt # library plotting\n","\n","# Get the IRIS dataset\n","url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n","data = pd.read_csv(url, names=['sepal length', 'sepal width', 'petal length', 'petal width', 'target'])\n","\n","# Set the figure size for the plot\n","plt.figure(figsize=(6, 6))\n","\n","# Create a scatter plot using seaborn\n","# - Set the data to data (original dataset)\n","# - Set the x-axis to 'sepal length' and y-axis to 'sepal width'\n","# - Use the 'target' variable for coloring different groups\n","# - Set the marker size to 60\n","# - Use the 'icefire' color palette for coloring\n","sb.scatterplot(data=data, x='sepal length', y='sepal width', hue='target', s=60, palette='icefire')\n"],"metadata":{"id":"nl9DS4J4z74E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we prepare the data and apply our own PCA functions."],"metadata":{"id":"XWzTuSbZ0LVn"}},{"cell_type":"code","source":["# Prepare the data\n","x = data.iloc[:, 0:4]  # Select the features (sepal length, sepal width, petal length, petal width)\n","\n","# Prepare the target\n","target = data.iloc[:, 4]  # Select the target variable (iris species)\n","\n","# Applying Principal Component Analysis (PCA) to reduce the dimensionality to 2 components\n","[mat_reduced, exp_variance ]= PCA_custom(x, 2)  # Your custom function\n","\n","# Creating a Pandas DataFrame of the reduced dataset\n","principal_df = pd.DataFrame(mat_reduced, columns=['PC1', 'PC2'])  # Create a DataFrame with the reduced components\n","\n","# Concatenate the reduced components with the target variable to create a complete dataset\n","principal_df = pd.concat([principal_df, pd.DataFrame(target)], axis=1)  # Concatenate the reduced components and target variable\n","\n","# Print explained variance ratios\n","for i, exp_var in enumerate(exp_variance[:2]):\n","    print(f\"Explained Variance for PC{i+1}: {exp_var*100:.2f}%\")"],"metadata":{"id":"grhhVQP2xEik"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's visualize the results. <p>\n","<strong>Task:</strong> Compare the original data with the dimension reduced data."],"metadata":{"id":"5JI83o1jzNnM"}},{"cell_type":"code","source":["import seaborn as sb\n","import matplotlib.pyplot as plt\n","\n","# Check the size of the data\n","print(\"Size of the original data: %s\\nSize of the dimensionality reduced data: %s\" % (x.shape, mat_reduced.shape))\n","\n","# Set the figure size for the plot\n","plt.figure(figsize=(6, 6))\n","\n","# Create a scatter plot using seaborn\n","# - Set the data to principal_df\n","# - Set the x-axis to 'PC1' and y-axis to 'PC2'\n","# - Use the 'target' variable for coloring different groups\n","# - Set the marker size to 60\n","# - Use the 'icefire' color palette for coloring\n","sb.scatterplot(data=principal_df, x='PC1', y='PC2', hue='target', s=60, palette='icefire')\n","\n","plt.xlabel(f'PC1: {exp_variance[0]*100:.2f}%')\n","plt.ylabel(f'PC2: {exp_variance[1]*100:.2f}%')"],"metadata":{"id":"BuUJj8iCzVdf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [Optional] Plot Dimensionality Reduced Data using 3 Prinicipal Components"],"metadata":{"id":"yRbkrqc8khPj"}},{"cell_type":"code","source":["# Applying Principal Component Analysis (PCA) to reduce the dimensionality to 3 components\n","[mat_reduced, exp_variance] = PCA_custom(x, 3)  # Your custom function\n","\n","# Creating a Pandas DataFrame of the reduced dataset\n","principal_df = pd.DataFrame(mat_reduced, columns=['PC1', 'PC2', 'PC3'])  # Create a DataFrame with the reduced components\n","\n","# Concatenate the reduced components with the target variable to create a complete dataset\n","principal_df = pd.concat([principal_df, pd.DataFrame(target)], axis=1)  # Concatenate the reduced components and target variable\n","\n","# Check the size of the data\n","print(\"Size of the original data: %s\\nSize of the dimensionality reduced data: %s\" % (x.shape, mat_reduced.shape))\n"],"metadata":{"id":"iIPZdAL_f3wZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder # to transform categorical labels to numerical labels for plotting\n","from matplotlib import colors as mcolors # for coloring\n","\n","le = LabelEncoder()\n","target_numerical = le.fit_transform(target)\n","\n","principal_df['target_numerical'] = target_numerical #Add this new numerical target to your DataFrame.\n","\n","\n","fig = plt.figure(figsize=(8, 8))\n","ax = fig.add_subplot(111, projection='3d')\n","\n","# Creating a colormap\n","my_cmap = mcolors.ListedColormap(['red', 'green', 'blue'])\n","\n","# Plotting the scatter plot\n","sctt = ax.scatter(principal_df['PC1'], principal_df['PC2'], principal_df['PC3'],\n","                  alpha=0.6,\n","                  c=principal_df['target_numerical'],  # Color by target_numerical\n","                  cmap=my_cmap,\n","                  marker='o')\n","\n","plt.title('3D scatter plot of PCA')\n","\n","ax.set_xlabel(f'PC1: {exp_variance[0]*100:.2f}%')\n","ax.set_ylabel(f'PC2: {exp_variance[1]*100:.2f}%')\n","ax.set_zlabel(f'PC3: {exp_variance[2]*100:.2f}%')\n","\n","# Add a color bar which maps values to colors.\n","fig.colorbar(sctt, ax = ax, ticks = range(len(le.classes_)), label = 'target')\n","\n","plt.show()\n"],"metadata":{"id":"U78zUjgYf_0E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Source: The step by step guide, the PCA function coded from scratch using Python and NumPy, as well as the application of the PCA function of the IRIS dataset is based on [this](https://www.askpython.com/python/examples/principal-component-analysis) guide."],"metadata":{"id":"c4HFEx21syrx"}},{"cell_type":"markdown","source":["### <center>Congratulations on completing the task! Well done! You have successfully learned how to write your own function from scratch to implement Principal Component Analysis (PCA) on a real-life dataset. </center>"],"metadata":{"id":"21L_U9F14wSi"}},{"cell_type":"markdown","source":["## **[OPTIONAL] Part 4: Apply your PCA Function on Your Own Dataset**\n","\n","Now it's time to apply what you have learned to your own dataset. Python offers a variety of open-source datasets that you can utilize, and we recommend leveraging one of those. However, you are also free to use other open-source datasets or data you have access to. Please note that while tutors may not be able to assist you with the specific dataset you choose, they can still guide you through the implementation process.<p>\n","\n","<strong>Task</strong>: Start by visualizing the data from your chosen dataset. Then, implement PCA using your own function. Experiment with different choices for the number of principal components to use for dimensionality reduction. Justify your selection of the number of principal components and observe how it impacts your results. Finally, visualize your results."],"metadata":{"id":"EQ5nUgkuuAiT"}},{"cell_type":"markdown","source":["### Finding your own dataset\n","Use the built-in datasets offered by scikit-learn to develop your own question to analyze using support vector machines. Follow the code below to learn how to import these datasets and display descriptive information to choose your favorite dataset. Use this time to get creative!"],"metadata":{"id":"yvLOwTfDbZuP"}},{"cell_type":"code","source":["from sklearn import datasets\n","\n","# List the available datasets:\n","dir(datasets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ThFoJEIbyHX","executionInfo":{"status":"ok","timestamp":1682023957891,"user_tz":300,"elapsed":1722,"user":{"displayName":"Morgan Fogarty","userId":"08379757262064945343"}},"outputId":"255cb93b-1f9a-4b67-f7ea-e643bb834a8c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['__all__',\n"," '__builtins__',\n"," '__cached__',\n"," '__doc__',\n"," '__file__',\n"," '__getattr__',\n"," '__loader__',\n"," '__name__',\n"," '__package__',\n"," '__path__',\n"," '__spec__',\n"," '_arff_parser',\n"," '_base',\n"," '_california_housing',\n"," '_covtype',\n"," '_kddcup99',\n"," '_lfw',\n"," '_olivetti_faces',\n"," '_openml',\n"," '_rcv1',\n"," '_samples_generator',\n"," '_species_distributions',\n"," '_svmlight_format_fast',\n"," '_svmlight_format_io',\n"," '_twenty_newsgroups',\n"," 'clear_data_home',\n"," 'dump_svmlight_file',\n"," 'fetch_20newsgroups',\n"," 'fetch_20newsgroups_vectorized',\n"," 'fetch_california_housing',\n"," 'fetch_covtype',\n"," 'fetch_kddcup99',\n"," 'fetch_lfw_pairs',\n"," 'fetch_lfw_people',\n"," 'fetch_olivetti_faces',\n"," 'fetch_openml',\n"," 'fetch_rcv1',\n"," 'fetch_species_distributions',\n"," 'get_data_home',\n"," 'load_breast_cancer',\n"," 'load_diabetes',\n"," 'load_digits',\n"," 'load_files',\n"," 'load_iris',\n"," 'load_linnerud',\n"," 'load_sample_image',\n"," 'load_sample_images',\n"," 'load_svmlight_file',\n"," 'load_svmlight_files',\n"," 'load_wine',\n"," 'make_biclusters',\n"," 'make_blobs',\n"," 'make_checkerboard',\n"," 'make_circles',\n"," 'make_classification',\n"," 'make_friedman1',\n"," 'make_friedman2',\n"," 'make_friedman3',\n"," 'make_gaussian_quantiles',\n"," 'make_hastie_10_2',\n"," 'make_low_rank_matrix',\n"," 'make_moons',\n"," 'make_multilabel_classification',\n"," 'make_regression',\n"," 'make_s_curve',\n"," 'make_sparse_coded_signal',\n"," 'make_sparse_spd_matrix',\n"," 'make_sparse_uncorrelated',\n"," 'make_spd_matrix',\n"," 'make_swiss_roll',\n"," 'textwrap']"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["# Output a discription of the dataset\n","print(datasets.load_digits().DESCR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S9_RNesZcQcM","executionInfo":{"status":"ok","timestamp":1682024053780,"user_tz":300,"elapsed":102,"user":{"displayName":"Morgan Fogarty","userId":"08379757262064945343"}},"outputId":"40802ed7-95b0-4b45-f477-f7beb15e27f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".. _digits_dataset:\n","\n","Optical recognition of handwritten digits dataset\n","--------------------------------------------------\n","\n","**Data Set Characteristics:**\n","\n","    :Number of Instances: 1797\n","    :Number of Attributes: 64\n","    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n","    :Missing Attribute Values: None\n","    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n","    :Date: July; 1998\n","\n","This is a copy of the test set of the UCI ML hand-written digits datasets\n","https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n","\n","The data set contains images of hand-written digits: 10 classes where\n","each class refers to a digit.\n","\n","Preprocessing programs made available by NIST were used to extract\n","normalized bitmaps of handwritten digits from a preprinted form. From a\n","total of 43 people, 30 contributed to the training set and different 13\n","to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n","4x4 and the number of on pixels are counted in each block. This generates\n","an input matrix of 8x8 where each element is an integer in the range\n","0..16. This reduces dimensionality and gives invariance to small\n","distortions.\n","\n","For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n","T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n","L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n","1994.\n","\n",".. topic:: References\n","\n","  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n","    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n","    Graduate Studies in Science and Engineering, Bogazici University.\n","  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n","  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n","    Linear dimensionalityreduction using relevance weighted LDA. School of\n","    Electrical and Electronic Engineering Nanyang Technological University.\n","    2005.\n","  - Claudio Gentile. A New Approximate Maximal Margin Classification\n","    Algorithm. NIPS. 2000.\n","\n"]}]},{"cell_type":"markdown","source":["## **Apply PCA on your Dataset**\n","Let's start by loading your dataset and visualizing the data before performing dimensionality reduction. You can follow the steps provided in Part 3 of the instructions. However, keep in mind that there are various ways to visualize data, so feel free to explore different techniques and choose the approach that best suits your needs. This flexibility allows you to showcase the characteristics and patterns of your dataset effectively. Once you have visualized the data, you will gain insights into its structure and distribution, which will serve as a reference for comparison after dimensionality reduction."],"metadata":{"id":"Yfs6QY2h0ksf"}},{"cell_type":"code","source":["# Visualization of the data set\n","\n","############ Insert your own code here ############"],"metadata":{"id":"e6iSVBAD0sgD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Perform PCA on your own data set using your own PCA function."],"metadata":{"id":"49_R3_430tbX"}},{"cell_type":"code","source":["# Data Prperation\n","\n","############ Insert your own code here ############\n","\n","# PCA\n","\n","############ Insert your own code here ############"],"metadata":{"id":"TtSjYS780tOo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualize your dimensionality reduced data. Compare your results to the initial dataset."],"metadata":{"id":"atCuFeAI0s0b"}},{"cell_type":"code","source":["# Visualization of the dimensionality reduced data\n","\n","############ Insert your own code here ############"],"metadata":{"id":"mW-IgBSP1A1n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### <center> You are an absolute Pro - well done! </center>"],"metadata":{"id":"kpu__B9Y2bB8"}}]}